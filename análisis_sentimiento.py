# -*- coding: utf-8 -*-
"""Análisis_sentimiento.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aD80KZQGXDqEZZ0j2jNVYkurBkfs92cm

# Estudio comparativo de técnicas clásicas, neuronales y basadas en Transformers para el Análisis de Sentimiento en reseñas cinematográficas de IMDb

En este trabajo se han explorado distintas líneas de Aprendizaje Automático aplicadas al Análisis de Sentimiento, con el objetivo de evaluar su eficacia en el corpus de reseñas cinematográficas IMDb.

Para ello se han empleado, tanto modelos clásicos como contemporáneos, que reflejan la evolución experimentada por esta área.

En una primera etapa, se han configurado clasificadores lineales con ponderaciones léxicas derivadas de SentiWordNet, calculadas mediante la suma y el promedio, con y sin reglas heurísticas. Además, se han entrenado modelos lineales (*Logistic Regression* con L1 y L2, y *RidgeClassifier*) tanto con n-gramas con *embeddings* preentrenados, como con características adicionales léxicas de SentiWordNet, para evaluar el impacto de incorporar un lexicón al modelo clasificatorio.

Posteriormente, se han entrenado modelos basados en redes neuronales (RNN y LSTM) con distintas variantes, como mecanismos de atención y arquitecturas multientrada, combinadas, asimismo, con características léxicas.

Finalmente, se han evaluado modelos de aprendizaje transferido basados en Transformers, en concreto, se ha realizado *fine-tuning* sobre el modelo RoBERTa(*Robustly Optimized BERT Approach)* base, con distintas combinaciones de hiperparámetros como la tasa de aprendizaje, el número de épocas y la penalización por regularización.

# 1. Primera parte: modelos clásicos

### 1.1. Importación de librerías, recursos, procesamiento del dataset y definición de variables de los conjuntos de entrenamiento y prueba

En esta sección se han importado las librerías necesarias para el desarrollo de modelos clásicos de aprendizaje automático aplicados al Análisis de sentimiento.

Se han incluido herramientas del Procesamiento del Lenguaje Natural (PLN) como NLTK, así como módulos de `scikit-learn` para la vectorización del texto, la construcción de pipelines, el ajuste de hiperparámetros y la evaluación de modelos.

Además, se han descargado los recursos lingüísticos necesarios de NLTK, como  `wordnet`, `sentiwordnet` y `stopwords`, que se emplearán para la lematización y la extracción de características léxicas.


Por último, se han importado herramientas para la interpretabilidad y visualización de modelos, como Lime, Seaborn o Matplotlib y funciones auxiliares de Keras para el preprocesamiento de secuencias que se realizarán en futuras etapas del proyecto.
"""

!pip install lime

import nltk
from nltk import word_tokenize, pos_tag
from nltk.corpus import wordnet as wn, sentiwordnet as swn, stopwords
from nltk.stem import WordNetLemmatizer


from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score, f1_score, hamming_loss, confusion_matrix, ConfusionMatrixDisplay
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline, FeatureUnion

from lime.lime_text import LimeTextExplainer


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import random
from IPython.display import display
import re, string

nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('sentiwordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('stopwords')

"""A continuación, se ha montado Google Drive y cargado el *Stanford Large Movie Review Dataset*, IMDb, corpus empleado en tareas de clasificación de sentimiento binario compuesto por 50.000 reseñas de películas procedentes de la plataforma IMDb.desde la ruta correspondiente. Además, se ha creado un DataFrame vacío para almacenar los resultados de los modelos evaluados.

"""

from google.colab import drive

drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Máster/IMDB Dataset.csv')
resultados_modelos = pd.DataFrame(columns=["modelo", "accuracy", "f1_macro"])

"""Posteriormente, se ha definido el lematizador `WordNetLemmatizer` de NLTK y la función `penn_to_wn`, que convierte las etiquetas del corpus Penn Treebank en las equivalentes de WordNet."""

lemmatizer = WordNetLemmatizer()

def penn_to_wn(tag):
    """
    Convertir etiquetas de PennTreebank en etiquetas de WordNet
    """
    if tag.startswith('J'):
        return wn.ADJ
    elif tag.startswith('N'):
        return wn.NOUN
    elif tag.startswith('R'):
        return wn.ADV
    elif tag.startswith('V'):
        return wn.VERB
    return None

"""Para mantener el balance de reseñas positivas y negativas y distribuir los datos en proporciones de un 80 % de reseñas para el conjunto de entrenamiento, 10 % para el de validación y 10 % para el de prueba, se ha aplicado `train_test_split` de Scikit-Learn con el parámetro `stratify=df['sentiment']` (que, además, realiza un `shuffle=True` para intercalar aleatoriamente filas positivas y negativas).

El proceso se ha verificado con la función `value_counts()`.
"""

train_df, rest_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df['sentiment']
)
valid_df, test_df = train_test_split(
    rest_df,
    test_size=0.5,
    random_state=42,
    stratify=rest_df['sentiment']
)

print("Train:", train_df['sentiment'].value_counts())
print("Valid:", valid_df['sentiment'].value_counts())
print("Test: ", test_df['sentiment'].value_counts())

"""A continuación, se han definido las variables necesarias para el entrenamiento y evaluación de los modelos clásicos de aprendizaje automático: `X_train` e `y_train` contienen, respectivamente, las reseñas y las etiquetas de sentimiento del conjunto de entrenamiento, mientras que `X_test` e `y_test` almacenan los datos equivalentes del conjunto de prueba.

"""

X_train = train_df["review"]
y_train = train_df["sentiment"]

X_test = test_df["review"]
y_test = test_df["sentiment"]

"""El gráfico confirma el equilibrio del dataset, con 20.000 reseñas para cada polaridad en el conjunto de entrenamiento."""

valores = train_df["sentiment"].value_counts(ascending=True)

ax = valores.plot.barh(color=["#373854","#9e379f"])

for i, valor in enumerate(valores):
    ax.text(valor, i, str(valor), va='center')

plt.title("Distribución de etiquetas de polaridad en el conjunto de entrenamiento")
plt.rcParams['figure.figsize'] = [10, 6]

plt.xlabel("Cantidad de reseñas")
plt.ylabel("Clase")
plt.tight_layout()
plt.show()

"""Además, se ha añadido a cada subconjunto una columna con el valor correspondiente, *train, valid o test,* de modo que pueda identificarse el origen de cada reseña y, mediante `full_df.sample(5)` se ha extraído una muestra aleatoria de cinco filas para comprobar que la combinación se ha realizado correctamente."""

train_df['split'] = 'train'
valid_df['split'] = 'valid'
test_df['split'] = 'test'
full_df = pd.concat([train_df, valid_df, test_df]).reset_index(drop=True)
full_df.sample(5)

"""## 2. Clasificador léxico basado en SentiWordNet: configuración básica y con reglas heurísticas

En esta primera parte se ha creado un clasificador de sentimiento con un lexicón basado en SentiWordNet para asignar una polaridad a cada reseña.

Se han definido dos configuraciones principales: una versión que únicamente suma o promedia las puntuaciones de positividad y negatividad extraídas de los *synsets*, y otra enriquecida con reglas heurísticas para capturar fenómenos lingüísticos como el empleo de intensificadores o negaciones.

Además, se ha experimentado con dos variantes: con el primer *synset* de SentiWordNet y con el promedio de las puntuaciones de todos ellos para evaluar cómo afecta este criterio de selección a la precisión del clasificador.

### 2.1 Modelo con lexicón basado en SentiWordNet con el primer *synset:* suma vs. media

Con estas dos funciones se ha definido un clasificador de sentimiento binario basado en el lexicón SentiWordNet, adaptado al caso de reseñas de IMDb que solo contempla dos polaridades.

Ambas funciones siguen los mismos pasos iniciales: tokenizan el texto, lo etiquetan gramaticalmente (POS-*tagging*), filtran las palabras que no pueden asociarse a ninguna categoría de WordNet y obtienen para cada lema la lista de *synsets* en SentiWordNet.

La función `Compute_sentiment_sum` acumula para cada palabra la diferencia entre su puntuación de positividad y de negatividad. Si la suma final es mayor que cero, se etiqueta como *positive*; si es menor, como *negative* y si es igual a cero, se recurre a una elección aleatoria entre ambas clases.

Por su parte, `compute_sentiment_mean` calcula la media aritmética de todas las palabras con *synsets*. La decisión de polaridad sigue el mismo criterio que la función anterior.

Ambas funciones incluyen un parámetro opcional *debug*, por defecto *False*, que, cuando se activa facilita el análisis de casos concretos.
"""

def compute_sentiment_sum(text, debug=False):
    tokenized_sent = nltk.word_tokenize(text)
    pos_tagged_tokens = nltk.pos_tag(tokenized_sent)

    sentiment_value = 0.0

    for word, tag in pos_tagged_tokens:
        wn_tag = penn_to_wn(tag)
        if wn_tag is None:
            continue

        lemma = lemmatizer.lemmatize(word.lower(), pos=wn_tag)
        swn_synsets = list(swn.senti_synsets(lemma, pos=wn_tag))
        if not swn_synsets:
            continue

        pos_score = swn_synsets[0].pos_score()
        neg_score = swn_synsets[0].neg_score()
        word_sentiment = pos_score - neg_score
        sentiment_value += word_sentiment

        if debug:
            print(f"{word} → lemma: {lemma}, POS: {wn_tag} | +{pos_score} -{neg_score} → {word_sentiment}")

    if sentiment_value == 0.0:
        sentiment_label = random.choice(["positive", "negative"])
        if debug:
            print("Score 0.0 → se elige al azar:", sentiment_label)
        return sentiment_label

    sentiment_label = "positive" if sentiment_value > 0 else "negative"

    if debug:
        print(f"Final score: {sentiment_value:.3f} → {sentiment_label}")

    return sentiment_label


def compute_sentiment_mean(text, debug=False):
    tokenized_sent = nltk.word_tokenize(text)
    pos_tagged_tokens = nltk.pos_tag(tokenized_sent)

    sentiment_values = []

    for word, tag in pos_tagged_tokens:
        wn_tag = penn_to_wn(tag)
        if wn_tag is None:
            continue

        lemma = lemmatizer.lemmatize(word.lower(), pos=wn_tag)
        swn_synsets = list(swn.senti_synsets(lemma, pos=wn_tag))
        if not swn_synsets:
            continue

        pos_score = swn_synsets[0].pos_score()
        neg_score = swn_synsets[0].neg_score()
        word_sentiment = pos_score - neg_score
        sentiment_values.append(word_sentiment)

        if debug:
            print(f"{word} → lemma: {lemma}, POS: {wn_tag} | +{pos_score} -{neg_score} → {word_sentiment}")

    if not sentiment_values:
        sentiment_label = random.choice(["positive", "negative"])
        if debug:
            print("Sin synsets válidos → se elige al azar:", sentiment_label)
        return sentiment_label

    mean_sentiment = sum(sentiment_values) / len(sentiment_values)

    if mean_sentiment == 0.0:
        sentiment_label = random.choice(["positive", "negative"])
        if debug:
            print("Score medio 0.0 → se elige al azar:", sentiment_label)
    else:
        sentiment_label = "positive" if mean_sentiment > 0 else "negative"
        if debug:
            print(f"Final mean score: {mean_sentiment:.3f} → {sentiment_label}")

    return sentiment_label

compute_sentiment_sum("This movie is not bad", debug=True)
compute_sentiment_mean("This movie is not bad", debug=True)

"""En el ejemplo se observa cómo, al no contemplarse la inversión de la negación, las palabras *not* y *bad* se tratan con una polaridad negativa, de modo que, tanto la suma final como la media, dan un resultado negativo.

Con la incorporación de reglas heurísticas se tratará de solventar este problema.

### 2. 2.  Clasificador léxico basado en SentiWordNet: configuración básica y con reglas heurísticas

Posteriormente, se han aplicado las funciones mencionadas a cada reseña del conjunto de prueba y sus predicciones se han guardado en `test_df`.

Para compararlas con las etiquetas reales, se han calculado las métricas *precision*, *recall* y *F1 score* y se han mostrado los resultados.
"""

test_df["lexico_sum_simple"] = test_df["review"].apply(compute_sentiment_sum)
test_df["lexico_mean_simple"] = test_df["review"].apply(compute_sentiment_mean)

clasificados_sum_simple = test_df[test_df["lexico_sum_simple"].notnull()]
print("Rendimiento del modelo lexicón primer synset (suma sin heurísticas):")
print(classification_report(clasificados_sum_simple["sentiment"], clasificados_sum_simple["lexico_sum_simple"]))

clasificados_mean_simple = test_df[test_df["lexico_mean_simple"].notnull()]
print("\nRendimiento del modelo lexicón primer synset (media sin heurísticas):")
print(classification_report(clasificados_mean_simple["sentiment"], clasificados_mean_simple["lexico_mean_simple"]))

"""### 2.3. Modelo con lexicón basado en SentiWordNet con el promedio de los *synsets*: suma vs. media

Estas dos funciones son idénticas a las anteriores pero, en lugar de tomar únicamente el primer *synset* de cada lema, calculan el promedio de las puntuaciones de positividad y negatividad de todos los *synsets* disponibles en SentiWordNet.

No se ha incorporado ningún mecanismo de desambiguación de sentido porque, aunque un *Word Sense Disambiguation* (WSD) permitiría seleccionar el *synset* más adecuado para cada palabra antes de extraer las puntuaciones de SentiWordNet, en experimentos previos no mejoró los resultados y habría incrementado la complejidad y el tiempo de ejecución del experimento.
"""

def compute_sentiment_sum_all_synsets(text, debug=False):
    tokenized_sent = nltk.word_tokenize(text)
    pos_tagged_tokens = nltk.pos_tag(tokenized_sent)

    sentiment_value = 0.0

    for word, tag in pos_tagged_tokens:
        wn_tag = penn_to_wn(tag)
        if wn_tag is None:
            continue

        lemma = lemmatizer.lemmatize(word.lower(), pos=wn_tag)
        swn_synsets = list(swn.senti_synsets(lemma, pos=wn_tag))
        if not swn_synsets:
            continue

        pos_avg = sum(s.pos_score() for s in swn_synsets) / len(swn_synsets)
        neg_avg = sum(s.neg_score() for s in swn_synsets) / len(swn_synsets)
        word_sentiment = pos_avg - neg_avg
        sentiment_value += word_sentiment

        if debug:
            print(f"{word} → lemma: {lemma}, POS: {wn_tag} | avg +{pos_avg:.3f} -{neg_avg:.3f} → {word_sentiment:.3f}")

    if sentiment_value == 0.0:
        sentiment_label = random.choice(["positive", "negative"])
        if debug:
            print("Score total 0.0 → se elige al azar:", sentiment_label)
        return sentiment_label

    sentiment_label = "positive" if sentiment_value > 0 else "negative"

    if debug:
        print(f"Final score: {sentiment_value:.3f} → {sentiment_label}")

    return sentiment_label

def compute_sentiment_mean_all_synsets(text, debug=False):
    tokenized_sent = nltk.word_tokenize(text)
    pos_tagged_tokens = nltk.pos_tag(tokenized_sent)

    sentiment_values = []

    for word, tag in pos_tagged_tokens:
        wn_tag = penn_to_wn(tag)
        if wn_tag is None:
            continue

        lemma = lemmatizer.lemmatize(word.lower(), pos=wn_tag)
        swn_synsets = list(swn.senti_synsets(lemma, pos=wn_tag))
        if not swn_synsets:
            continue

        pos_avg = sum(s.pos_score() for s in swn_synsets) / len(swn_synsets)
        neg_avg = sum(s.neg_score() for s in swn_synsets) / len(swn_synsets)
        word_sentiment = pos_avg - neg_avg
        sentiment_values.append(word_sentiment)

        if debug:
            print(f"{word} → lemma: {lemma}, POS: {wn_tag} | avg +{pos_avg:.3f} -{neg_avg:.3f} → {word_sentiment:.3f}")

    if not sentiment_values:
        sentiment_label = random.choice(["positive", "negative"])
        if debug:
            print("Sin synsets válidos → se elige al azar:", sentiment_label)
        return sentiment_label

    mean_sentiment = sum(sentiment_values) / len(sentiment_values)

    if mean_sentiment == 0.0:
        sentiment_label = random.choice(["positive", "negative"])
        if debug:
            print("Score medio 0.0 → se elige al azar:", sentiment_label)
    else:
        sentiment_label = "positive" if mean_sentiment > 0 else "negative"
        if debug:
            print(f"Final mean score: {mean_sentiment:.3f} → {sentiment_label}")

    return sentiment_label

test_df["lexico_sum_all_synsets_base"] = test_df["review"].apply(compute_sentiment_sum_all_synsets)
test_df["lexico_mean_all_synsets_base"] = test_df["review"].apply(compute_sentiment_mean_all_synsets)

# Evaluar modelo de suma (todos los synsets promediados)
clasificados_sum_base = test_df[test_df["lexico_sum_all_synsets_base"].notnull()]
print("Rendimiento del modelo lexicón (suma sin heurísticas, todos los synsets):")
print(classification_report(clasificados_sum_base["sentiment"], clasificados_sum_base["lexico_sum_all_synsets_base"]))

# Evaluar modelo de media (todos los synsets promediados)
clasificados_mean_base = test_df[test_df["lexico_mean_all_synsets_base"].notnull()]
print("\nRendimiento del modelo lexicón (media sin heurísticas, todos los synsets):")
print(classification_report(clasificados_mean_base["sentiment"], clasificados_mean_base["lexico_mean_all_synsets_base"]))

"""### 2. 4. Comparativa de los modelos con lexicón sin reglas heurísticas

| Configuración                                       | Accuracy | Macro F1 | Recall Positivo | Recall Negativo |
|-----------------------------------------------------|----------|----------|------------------|------------------|
| Primer synset - suma                                | 0.61     | 0.58     | 0.89             | 0.34             |
| Primer synset - media                               | 0.61     | 0.58     | 0.89             | 0.34             |
| Todos los synsets - suma                            | 0.69     | **0.68**     | 0.83             | 0.56             |
| Todos los synsets - media                           | 0.69     | **0.68**     | 0.83             | 0.56             |




Al comparar las variantes sin heurísticas, el modelo que promedia todos los *synsets* de SentiWordNet obtiene un *Macro F1* de 0.68 (*accuracy* 0.69), mientras que la versión que se limita al primer *synset* alcanza un *Macro F1* de 0.58 .

Esta diferencia se aprecia especialmente en la clase negativa, cuyo *F1* pasa de 0.47 a 0.64, por lo que se deduce que considerar todos los *synsets* mejora notablemente el rendimiento del modelo.

Además, se observa que no hay diferencia entre utilizar la suma o la media de los valores de polaridad, ya que la decisión final del modelo depende del signo, no de su magnitud.

### 2. 5. Modelo con reglas heurísticas con lexicón de SentiWordNet: suma vs. media con promedio de *synsets*

Dado que en las pruebas anteriores se ha revelado un desempeño inferior con la configuración del lexicón con solo el primer *synset*, en este modelo se ha optado por su promedio.

Además, para su creación se ha definido un conjunto de palabras que denotan negación y un diccionario de modificadores de intensidad que aúna intensificadores y atenuadores con sus correspondientes ponderaciones.

A continuación, se han definido las funciones `compute_sentiment_sum_heuristic` y `compute_sentiment_mean_heuristic`, que combinan el promedio de valores de todos los *synsets* de SentiWordNet con dos reglas heurísticas: en una ventana de contexto de 3, 4 y 5 tokens anteriores se invierte la polaridad de la puntuación, si se detecta una negación (pues anteriormente se había comprobado cómo su omisión conducía a un etiquetado incorrecto); y si el término anterior se encuentra entre los `intensity_modifiers`, se multiplica su ponderación por el factor correspondiente.

La primera función acumula esas diferencias como un sumatorio global, mientras que la segunda las agrega a una lista para luego calcular su media.

Como en los anteriores casos, si no se encuentra ninguna palabra con *synsets* o el resultado es exactamente cero, en ambas funciones se recurre a una elección aleatoria de categoría.
"""

negations = {"not", "no", "never", "n't", "none", "cannot", "doesn't", "don't", "neither", "nor"}
intensity_modifiers = {
    "very": 1.5, "extremely": 1.8, "quite": 1.2, "really": 1.3, "absolutely": 1.8,
    "so": 1.2, "highly": 1.4, "incredibly": 1.5,
    "slightly": 0.8, "barely": 0.5, "hardly": 0.5, "somewhat": 0.7, "marginally": 0.6
}


def compute_sentiment_sum_heuristic(text, window=5, debug=False):
    tokenized_sent = nltk.word_tokenize(text)
    pos_tagged_tokens = nltk.pos_tag(tokenized_sent)

    sentiment_value = 0.0
    sentiment_count = 0

    for i, (word, tag) in enumerate(pos_tagged_tokens):
        wn_tag = penn_to_wn(tag)
        if wn_tag is None:
            continue

        lemma = lemmatizer.lemmatize(word.lower(), pos=wn_tag)
        swn_synsets = list(swn.senti_synsets(lemma, pos=wn_tag))
        if not swn_synsets:
            continue

        pos_avg = sum(s.pos_score() for s in swn_synsets) / len(swn_synsets)
        neg_avg = sum(s.neg_score() for s in swn_synsets) / len(swn_synsets)
        word_sentiment = pos_avg - neg_avg

        # Heurística de negación (ventana anterior)
        window_start = max(0, i - window)
        context = pos_tagged_tokens[window_start:i]
        if any(tok[0].lower() in negations for tok in context):
            word_sentiment *= -1

        # Heurística de intensificadores/atenuadores
        if i > 0:
            prev_word = pos_tagged_tokens[i - 1][0].lower()
            if prev_word in intensity_modifiers:
                word_sentiment *= intensity_modifiers[prev_word]

        sentiment_value += word_sentiment
        sentiment_count += 1

        if debug:
            print(f"{word} → sentiment: {word_sentiment:.3f}")

    if sentiment_count == 0 or sentiment_value == 0.0:
        sentiment_label = random.choice(["positive", "negative"])
        if debug:
            print("Sin sentimiento válido o score 0.0 → al azar:", sentiment_label)
        return sentiment_label

    sentiment_label = "positive" if sentiment_value > 0 else "negative"
    if debug:
        print(f"Final score: {sentiment_value:.3f} → {sentiment_label}")
    return sentiment_label
def compute_sentiment_mean_heuristic(text, window=5, debug=False):
    tokenized_sent = nltk.word_tokenize(text)
    pos_tagged_tokens = nltk.pos_tag(tokenized_sent)

    sentiment_values = []

    for i, (word, tag) in enumerate(pos_tagged_tokens):
        wn_tag = penn_to_wn(tag)
        if wn_tag is None:
            continue

        lemma = lemmatizer.lemmatize(word.lower(), pos=wn_tag)
        swn_synsets = list(swn.senti_synsets(lemma, pos=wn_tag))
        if not swn_synsets:
            continue

        pos_avg = sum(s.pos_score() for s in swn_synsets) / len(swn_synsets)
        neg_avg = sum(s.neg_score() for s in swn_synsets) / len(swn_synsets)

        # Heurística de negación
        window_start = max(0, i - window)
        context = pos_tagged_tokens[window_start:i]
        if any(tok[0].lower() in negations for tok in context):
            pos_avg, neg_avg = neg_avg, pos_avg  # invertir

        # Heurística de intensificación/atenuación
        if i > 0:
            prev_word = pos_tagged_tokens[i - 1][0].lower()
            if prev_word in intensity_modifiers:
                factor = intensity_modifiers[prev_word]
                pos_avg *= factor
                neg_avg *= factor

        word_sentiment = pos_avg - neg_avg
        sentiment_values.append(word_sentiment)

        if debug:
            print(f"{word} → pos: {pos_avg:.3f}, neg: {neg_avg:.3f} → sentiment: {word_sentiment:.3f}")

    if not sentiment_values:
        sentiment_label = random.choice(["positive", "negative"])
        if debug:
            print("Sin sentimiento válido → al azar:", sentiment_label)
        return sentiment_label

    mean_sentiment = sum(sentiment_values) / len(sentiment_values)

    if mean_sentiment == 0.0:
        sentiment_label = random.choice(["positive", "negative"])
        if debug:
            print("Score medio 0.0 → al azar:", sentiment_label)
    else:
        sentiment_label = "positive" if mean_sentiment > 0 else "negative"
        if debug:
            print(f"Final mean score: {mean_sentiment:.3f} → {sentiment_label}")

    return sentiment_label

for w in [3, 4, 5]:

    # Suma con heurísiticas y promedio de todos los synsets
    test_df[f"swn_sum_all_w{w}"] = test_df["review"].apply(
        lambda x: compute_sentiment_sum_heuristic(x, window=w)
    )

    # Media con heurísticas y promedio de todos los synsets
    test_df[f"swn_mean_all_w{w}"] = test_df["review"].apply(
        lambda x: compute_sentiment_mean_heuristic(x, window=w)
    )

"""A continuación, para evaluar cómo varía el rendimiento de las dos configuraciones heurísticas (suma y media con promedio de *synsets*), al cambiar el tamaño de la ventana de negación, para cada valor de w en 3, 4 y 5 tokens, el bucle imprime el informe de precisión, *recall* y *F1-score*, tanto para la versión del sumatorio como para la de la media."""

for w in [3, 4, 5]:

    print(f"\nResultados del modelo CON heurísticas (SUMA + promedio de synsets) - Ventana {w}:")
    clasificados_sum_all = test_df[test_df[f"swn_sum_all_w{w}"].notnull()]
    print(classification_report(clasificados_sum_all["sentiment"], clasificados_sum_all[f"swn_sum_all_w{w}"]))

    print(f"\nResultados del modelo CON heurísticas (MEDIA + promedio de synsets) - Ventana {w}:")
    clasificados_mean_all = test_df[test_df[f"swn_mean_all_w{w}"].notnull()]
    print(classification_report(clasificados_mean_all["sentiment"], clasificados_mean_all[f"swn_mean_all_w{w}"]))

"""### 2. 6. Análisis de resultados globales


 | Configuración                             | Accuracy | Macro F1 |
|------------------------------------------|----------|----------|
| Primer synset - suma                     | 0.61     | 0.58     |
| Primer synset - media                    | 0.61     | 0.58     |
| Todos los synsets - suma                 | 0.69     | 0.68     |
| Todos los synsets - media                | 0.69     | 0.68     |
 **Suma heurísticas (ventana 3)**              | **0.69**     | **0.69**     |
| **Media heurísticas (ventana 3)**             | **0.69**     | **0.69**     |
| **Suma heurísticas (ventana 4)**              | **0.69**     | **0.69**     |
| **Media heurísticas (ventana 4)**             | **0.69**     | **0.69**     |
| **Suma heurísticas (ventana 5)**              | **0.69**     | **0.69**     |
| **Media heurísticas (ventana 5)**             | **0.69**     | **0.69**     |



Del análisis comparativo se desprende que en todos los modelos con reglas heurísticas, tanto la versión de la suma, como la de la media de *synsets* y en el rango de 3, 4 y 5 ventanas contextuales, el valor del Macro-F1 se mantiene constante en 0.69, pues, a pesar de que hay ligeras variaciones en el *recall* de la clase *negative* y, consecuentemente, en la *precision*, el balance global implica que no varíe.

Asimismo, la incorporación de las reglas de negación e intensificadores aporta, una ligera mejora (+0.01) con respecto a los modelos de lexicón sin reglas heurísticas.

Además, en comparación con el enfoque más simple, basado únicamente en el primer *synset* de cada término, que obtiene un Macro-F1 de 0.58, la mejora es considerable.

Por ello se toma como mejor modelo de esta primera tarea el basado en el lexicón con *synsets* promediados, cálculo de la suma (la media no mejora y es más compleja de calcular) y reglas heurísticas en una ventana de 3 tokens, ya que la variación de ventana no ha implicado mejores resultados y puede empeorar la eficiencia.

Debe tenerse en cuenta que, tanto la versión de la suma como la de media coinciden en los resultados porque la clasificación se basa en el signo de la polaridad y la media no lo varía, luego podrían haberse omitido esas pruebas.

### 2. 7. Análisis cualitativo: predicciones de sentimiento del mejor modelo con reglas heurísticas y agregación por suma en una ventana de 3 tokens sobre reseñas de IMDb

A continuación se presentan algunas reseñas del conjunto de prueba, junto con las predicciones generadas por el mejor modelo léxico, basado en SentiWordNet, con reglas heurísticas y agregación mediante suma en una ventana de 3 tokens.

Dado que los resultados obtenidos con ventanas de 4 y 5 tokens, así como los modelos basados en el promedio, han sido equivalentes en rendimiento, se han omitido sus predicciones para evitar redundancias.
"""

pd.set_option('display.max_colwidth', None)

for w in [3, 4, 5]:
    test_df[f"pred_sentiment_swn_sum_w{w}"] = test_df["review"].apply(
        lambda x: compute_sentiment_sum_heuristic(x, window=w)
    )

    test_df[f"pred_sentiment_swn_mean_w{w}"] = test_df["review"].apply(
        lambda x: compute_sentiment_mean_heuristic(x, window=w)
    )

    if "sentiment" in test_df.columns:
        test_df[f"acierto_sum_w{w}"] = test_df[f"pred_sentiment_swn_sum_w{w}"] == test_df["sentiment"]
        test_df[f"acierto_mean_w{w}"] = test_df[f"pred_sentiment_swn_mean_w{w}"] == test_df["sentiment"]


test_df[[
    "review", "sentiment",
    "pred_sentiment_swn_sum_w3"
]].head(4)

"""#### 2. 7. 1. Instancias 46799 y 45891

Estas instancias son ejemplos claros de las limitaciones de un enfoque clásico basado en lexicón, ya que, a pesar de la aplicación de reglas heurísticas para detectar negaciones e intensificadores, el modelo interpreta erróneamente reseñas repletas de signos de polaridad contrarios a la predicha.

De esta manera, a pesar de que la crítica numerada como 46799 presenta expresiones con una polaridad inequívocamente negativa: "...the complete waste that comprises the entire plot of the movie"; "..complete waste of time that was spent watching the movie..."; adverbios valorativos con connotación negativa como *unfortunately*, e incluso su emisor recurre a un tono sarcástico con el empleo de voces referidas al *dolor*, "If you did end up seeing this movie, I understand your pain :)", el modelo clasifica esta reseña como positiva.

Similar caso es el de la reseña 45891 que, aun con frases altamente positivas desde el inicio: "THE BLOB is a great horror movie", "A very good film... a must-see"; "perfect tonic for the kind of depression..."; "innovative (and effective!) visual effects", o "undeniably clever", que incluyen adjetivos valorativos que connotan sentimiento positivo, como *great, clever, good* (con el intensificador *very*), o *effective!* (enfatizado, además, mediante la modalidad exclamativa), el modelo la clasifica como negativa.

Este tipo de errores pueden explicarse, además de por las limitaciones estructurales del diseño del modelo (como el hecho de que se base en puntuaciones promedio de los distintos *synsets* asociados a una palabra, por lo que puede asignar valores de polaridad inexactos), por otras, inherentes a SentiWordNet, como la magnitud limitada del lexicón.

En definitiva, a pesar de la existencia de reglas heurísticas que tratan de captar cierto contexto, la evaluación palabra por palabra, se revela insuficiente para captar aspectos semántico-discursivos a nivel de frase, así como expresiones irónicas, cuyo significado no puede derivarse de la suma aislada de voces o de una ventana de 3 tokens determinada por reglas heurísticas.

### 3. Clasificadores basados en Regresión Logística

La Regresión Logística es uno de los algoritmos más utilizados en tareas de clasificación binaria como el Análisis de Sentimiento debido a su simplicidad y eficiencia.

En este trabajo se han combinado distintas variantes de este clasificador: Regresión Logística con penalización L1 (Lasso), L2 (Ridge), así como el modelo *RidgeClassifier,* que optimiza una función de pérdida cuadrática.

Para evaluar su rendimiento, se han empleado varias estrategias de vectorización textual (CountVectorizer y TfidfVectorizer), distintos rangos de n-gramas ((2,2), (3,3) y (2,3)), y dos formas de preprocesamiento léxico: tokenización y lematización.

### 3.1 Entrenamiento y evaluación de 36 modelos de Regresión Logística: vectorización, preprocesamiento y regularización

En primer lugar, se han definido dos funciones de preprocesado para los modelos de Regresión Logística basados en n-gramas.

Ambas se emplearán para construir las matrices de características de  n-gramas en las dos variantes: una con tokenización  y otra con lematización previa.
"""

stop_words = set(stopwords.words("english"))

def tokenizar(texto):
    tokens = word_tokenize(texto)
    return [t.lower().strip() for t in tokens if t.strip()]

def lematizar(texto):
    tokens = word_tokenize(texto)
    return [
        lemmatizer.lemmatize(t.lower().strip())
        for t in tokens
        if t.lower() not in stop_words and t.isalpha()
    ]

"""En esta celda se configura el entrenamiento y evaluación de un total de 36 modelos de aprendizaje automático clásico mediante un bucle que recorre todas las combinaciones posibles entre:

-Dos vectorizadores: `CountVectorizer` y `TfidfVectorizer`.

-Dos funciones de preprocesamiento: tokenización básica y lematización.

-Tres rangos de n-gramas: bigramas `(2,2)`, trigramas `(3,3)` y combinación `(2,3)`.

-Tres modelos de clasificación: `LogisticRegression` con regularización L1 (Lasso), `LogisticRegression` con regularización L2 (Ridge) y `RidgeClassifier`

Para cada combinación, se ha instanciado el vectorizador correspondiente (con `min_df=3` para eliminar términos poco frecuentes), se han transformado las reseñas de entrenamiento y prueba (`X_train_vec`, `X_test_vec`), y se ha entrenado el modelo correspondiente.

Al finalizar, se ha creado un `DataFrame` con todos los resultados, que se ordena según el valor de `Macro_F1`.

Dado que el conjunto de datos está perfectamente balanceado, no ha sido necesario realizar ningún ajuste adicional en cuanto a pesos de clase.

"""

resultados = []

vectorizadores = {
    'CountVectorizer': CountVectorizer,
    'TfidfVectorizer': TfidfVectorizer
}

tokenizadores = {
    'tokenizacion': tokenizar,
    'lematizacion': lematizar
}

ngramas = [(2, 2), (3, 3), (2, 3)]

modelos = {
    "LR L1 (Lasso)": LogisticRegression(penalty='l1', solver='saga', multi_class='multinomial', max_iter=1000, C=0.1),
    "LR L2 (Ridge)": LogisticRegression(penalty='l2', solver='saga', multi_class='multinomial', max_iter=1000, C=1),
    "RidgeClassifier": RidgeClassifier(alpha=1.0)
}

for vec_name, Vec in vectorizadores.items():
    for tok_name, tok_func in tokenizadores.items():
        for ngram_range in ngramas:
            print(f"\n=== {vec_name} | {tok_name} | ngram_range={ngram_range} ===")

            vectorizer = Vec(
                tokenizer=tok_func,
                ngram_range=ngram_range,
                min_df=3
            )

            X_train_vec = vectorizer.fit_transform(X_train)
            X_test_vec = vectorizer.transform(X_test)


            for modelo_nombre, modelo in modelos.items():
                print(f"→ Modelo: {modelo_nombre}")

                modelo.fit(X_train_vec, y_train)
                y_pred = modelo.predict(X_test_vec)

                print(classification_report(y_test, y_pred, zero_division=0))


                resultados.append({
                    "Vectorizador": vec_name,
                    "Procesamiento": tok_name,
                    "Ngrama": ngram_range,
                    "Modelo": modelo_nombre,
                    "Macro_F1": f1_score(y_test, y_pred, average='macro', zero_division=0),
                    "Accuracy": accuracy_score(y_test, y_pred),
                    "Subset_Accuracy": accuracy_score(y_test, y_pred),
                    "Hamming_Loss": hamming_loss(y_test, y_pred)
                })

# Mostrar resultados finales
df_resultados = pd.DataFrame(resultados)
df_resultados_ordenado = df_resultados.sort_values(by="Macro_F1", ascending=False)

print("\n==== TABLA DE RESULTADOS ORDENADA POR MACRO F1 ====")
display(df_resultados_ordenado)

print("\nMEJOR Macro F1:")
print(df_resultados_ordenado.iloc[0])

print("\nMEJOR Accuracy:")
print(df_resultados.sort_values(by="Accuracy", ascending=False).iloc[0])

print("\nMEJOR Subset Accuracy:")
print(df_resultados.sort_values(by="Subset_Accuracy", ascending=False).iloc[0])

print("\nMENOR Hamming Loss:")
print(df_resultados.sort_values(by="Hamming_Loss", ascending=True).iloc[0])

"""De los resultados obtenidos con las combinaciones evaluadas se observa un patrón: los mejores resultados se alcanzan con la combinación de`TfidfVectorizer` tokenización simple, y el modelo `RidgeClassifier`.

En cuanto al rango de n-gramas, aunque los bigramas `(2,2)` han ofrecido un buen rendimiento, la mejor configuración se corresponde con el empleo combinado de bigramas y trigramas `(2,3)`.

La combinación del vectorizador TF-IDF, junto con la tokenización, (2,3) gramas y el modelo *RidgeClassifier* logra un *Macro-F1* de 0.9180, resultado excelente para tratarse de un modelo clásico de clasificación, aun cuando esta sea binaria.

| Vectorizador    | Procesamiento | Ngrama   | Modelo         | Macro F1 | Accuracy | Subset Accuracy | Hamming Loss |
|-----------------|----------------|----------|----------------|----------|----------|------------------|---------------|
| TfidfVectorizer | tokenizacion   | (2, 3)   | RidgeClassifier | **0.9180**   | 0.9180   | 0.9180           | 0.0820        |

### 3.2. Búsqueda de la optimización del mejor modelo *RidgeClassifier*

 Con el objetivo de optimizar el rendimiento del mejor modelo *RidgeClassifier*, se ha realizado una búsqueda basada en las distintas configuraciones que, a continuación, se muestran.

### 3. 2. 1. Mediante el hiperparámetro `alpha`
"""

vectorizer = TfidfVectorizer(tokenizer=tokenizar, ngram_range=(2, 3), min_df=3)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)


param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}

gs = GridSearchCV(
    RidgeClassifier(),
    param_grid,
    scoring='f1_macro',
    cv=5,
    n_jobs=-1
)


gs.fit(X_train_vec, y_train)


print("Mejor alpha:", gs.best_params_['alpha'])
print("Mejor F1_macro CV:", gs.best_score_)

y_pred = gs.predict(X_test_vec)
print("F1_macro test:", f1_score(y_test, y_pred, average='macro'))

"""La búsqueda de hiperparámetros sobre el modelo `RidgeClassifier` con la mejor configuración previa (TF-IDF + tokenización + (2,3) gramas ) ha confirmado que `alpha=1.0` es el valor óptimo, ya que no se ha obtenido ninguna mejora al evaluar otros valores.

### 3.2. 2. Entrenamiento del mejor modelo *RidgeClassifier* sin lexicón con manejo de negaciones

Debido a que en tareas de clasificación de Análisis de Sentimiento las construcciones negativas invierten la polaridad, se ha creado una función de preprocesado sin lematización que detecta marcadores de negación y antepone *not* a las palabras posteriores. Con este tratamiento de las negaciones se ha entrenado de nuevo el mejor modelo obtenido hasta el momento.
"""

def preprocesar_tokenizar(texto):

    tokens = word_tokenize(texto.lower())

    neg_pattern = re.compile(r'^(no|not|never|n’t|don’t|doesn’t|can’t|couldn’t|wouldn’t|shouldn’t|mustn’t)$')
    salida = []
    negado = False

    for token in tokens:
        if token in string.punctuation:
            negado = False
            continue
        if neg_pattern.match(token):
            negado = True
            salida.append(token)
        elif negado:
            salida.append("not_" + token)
        else:
            salida.append(token)

    return [
        t for t in salida
        if t not in stop_words and t.isalpha()
    ]

vectorizer = TfidfVectorizer(
    tokenizer=tokenizar,
    ngram_range=(2, 3),
    min_df=3
)

X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec  = vectorizer.transform(X_test)


model = RidgeClassifier(alpha=1.0)
model.fit(X_train_vec, y_train)
y_pred = model.predict(X_test_vec)



print(classification_report(y_test, y_pred, zero_division=0))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1 macro:", f1_score(y_test, y_pred, average='macro'))

"""El modelo con preprocesamiento de negación y el modelo clásico optimizado (`RidgeClassifier` con TF-IDF y n-gramas (2,3)) alcanzan exactamente el mismo rendimiento: `Macro F1 = 0.917998` y `Accuracy = 0.918`.

### 3.2. 3. Entrenamiento del mejor modelo *RidgeClassifier* con lexicón basado en SentiWordNet

Para entrenar la mejor combinación obtenida de Regresión Logística con lexicón, se han definido las funciones de preprocesado que transforman el lenguaje para que cada término del dataset coincida con su *synset* correspondiente en WordNet.
"""

def tokenizar(texto):
    tokens = word_tokenize(texto)
    return [t.lower().strip() for t in tokens if t.strip()]

def lematizar(texto):
    tokens = word_tokenize(texto)
    return [
        lemmatizer.lemmatize(t.lower().strip())
        for t in tokens
        if t.lower() not in stop_words and t.isalpha()
    ]

""" ### 3.2. 3. 1.  Extracción de características de sentimiento con SentiWordNet

Se han calculado dos características por reseña, `swn_pos_sum y swn_neg_sum` que acumulan, respectivamente, la media de los puntajes de positividad y de negatividad obtenidos de todos los *synsets* de cada lema en SentiWordNet.

Estas variables permiten que el clasificador adapte su frontera de decisión sobre el plano, en lugar de basarse en un único umbral unidimensional.

Para su obtención, primero se ha tokenizado el texto y se ha etiquetado cada token con su categoría gramatical de WordNet; a continuación, se han descartado los términos que no han dispuesto de mapeo válido, se han lematizado los restantes y se han promediado los puntajes positivos y negativos de todos sus *synsets.*

El resultado es un vector bidimensional que aporta señal afectiva complementaria a las representaciones de n-gramas.

Asimismo, se ha elegido  promediar todos los *synsets*, en lugar de tomar solo el más frecuente, ya que en los modelos más simples anteriormente analizados esta estrategia ha alcanzado mejor rendimiento.


"""

class SentiWordNetScoreExtractor2(BaseEstimator, TransformerMixin):
    def __init__(self):
        #
        self.feature_names = ["swn_pos_sum", "swn_neg_sum"]

    def fit(self, X, y=None):
        return self

    def get_feature_names_out(self, input_features=None):
        return np.array(self.feature_names)

    def transform(self, X):
        features = []
        for text in X:
            pos_sum, neg_sum = 0.0, 0.0
            tokens = word_tokenize(text.lower())
            tagged = pos_tag(tokens)

            for word, tag in tagged:
                wn_tag = penn_to_wn(tag)
                if wn_tag is None:
                    continue

                lemma = lemmatizer.lemmatize(word, wn_tag)
                synsets = list(swn.senti_synsets(lemma, wn_tag))
                if not synsets:
                    continue


                pos_sum += np.mean([s.pos_score() for s in synsets])
                neg_sum += np.mean([s.neg_score() for s in synsets])

            features.append([pos_sum, neg_sum])

        return np.array(features)

""" ### 3.3. Entrenamiento del mejor modelo *RidgeClassifier* con n-gramas vs. n-gramas y características léxicas

En este bloque se ha evaluado el impacto de incorporar características léxicas derivadas de SentiWordNet al mejor modelo previamente identificado (`RidgeClassifier` con `TfidfVectorizer`, (2,3) gramas  y tokenización simple).

Para ello, se han comparado dos configuraciones:

-Una basada únicamente en la representación de n-gramas.

-Otra que combina n-gramas con dos rasgos adicionales (`swn_pos_sum`, `swn_neg_sum`), que representan la media de las puntuaciones de positividad y negatividad de todos los *synsets* de cada palabra en SentiWordNet.

Finalmente, sus métricas se han almacenado para su análisis comparativo.

"""

resultados = []

token_func = tokenizar
vectorizador = TfidfVectorizer(
    tokenizer=token_func,
    ngram_range=(2, 3),
    min_df=3,
    token_pattern=None
)

# 1. Sin características léxicas
pipe_sin_lexico = Pipeline([
    ('vect', vectorizador),
    ('clf', RidgeClassifier(alpha=1.0))
])

pipe_sin_lexico.fit(X_train, y_train)
y_pred_sin = pipe_sin_lexico.predict(X_test)

resultados.append({
    "Configuración": "Sin atributos léxicos",
    "Macro_F1": f1_score(y_test, y_pred_sin, average='macro', zero_division=0),
    "Accuracy": accuracy_score(y_test, y_pred_sin),
    "Hamming_Loss": hamming_loss(y_test, y_pred_sin)
})

# 2. Con características léxicas
pipe_con_lexico = Pipeline([
    ('features', FeatureUnion([
        ('vect', vectorizador),
        ('lexico', SentiWordNetScoreExtractor2())
    ])),
    ('clf', RidgeClassifier(alpha=1.0))
])

pipe_con_lexico.fit(X_train, y_train)
y_pred_con = pipe_con_lexico.predict(X_test)

resultados.append({
    "Configuración": "Con atributos léxicos",
    "Macro_F1": f1_score(y_test, y_pred_con, average='macro', zero_division=0),
    "Accuracy": accuracy_score(y_test, y_pred_con),
    "Hamming_Loss": hamming_loss(y_test, y_pred_con)
})


df = pd.DataFrame(resultados).sort_values("Macro_F1", ascending=False)
display(df)

"""Al comparar el mejor modelo con el que presenta características léxicas derivadas de SentiWordNet, se observa que, a pesar de que ambos modelos obtienen buenos resultados, la versión sencilla supera a la creada con lexicón en todas las métricas: `Macro F1 = 0.917998` frente a `0.910598`, `Accuracy = 0.9180` frente a `0.9106`, y un menor `Hamming Loss`.

### 3.4. Análisis de los resultados

Tras la evaluación de las configuraciones mostradas, se concluye que la versión simple del modelo ha ofrecido el mejor rendimiento global:  *RidgeClassifier* entrenado con TfidfVectorizer, (2,3-gramas ) y tokenización.

En cuanto a los valores del hiperparámetro *alpha,* se ha determinado que un valor igual 1 proporciona una regularización adecuada.

Por otro lado, si bien la incorporación de negaciones ha mantenido los mejores resultados, la adición de características léxicas derivadas de SentiWordNet no los ha superado, por lo que para el corpus IMDb una representación basada exclusivamente en n-gramas y TF-IDF es suficientemente eficaz.

### 3. 5. Análisis cualitativo: predicciones de sentimiento del mejor modelo,  *RidgeClassifier* entrenado con TfidfVectorizer, (2,3-gramas ) y tokenización sobre reseñas de IMDb
"""

vectorizer = TfidfVectorizer(
    tokenizer=tokenizar,
    ngram_range=(2, 3),
    min_df=3
)

X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

model = RidgeClassifier(alpha=1.0)
model.fit(X_train_vec, y_train)

y_pred = model.predict(X_test_vec)


pd.set_option('display.max_colwidth', None)


df_test = pd.DataFrame({
    "review": X_test,
    "sentiment": y_test,
    "pred_ridge": y_pred
})

df_test["acierto_ridge"] = df_test["pred_ridge"] == df_test["sentiment"]

print("\n Ejemplos clasificados diferentemente:")
display(df_test[df_test["acierto_ridge"] == False][["review", "sentiment", "pred_ridge"]].sample(2, random_state=42))

print("\n Ejemplos clasificados igualmente:")
display(df_test[df_test["acierto_ridge"] == True][["review", "sentiment", "pred_ridge"]].sample(2, random_state=42))

"""#### 3. 5. 1. Reseñas 4343 y 23262

La reseña 4343 es un buen ejemplo de la dificultad de clasificar ciertas críticas debido a su contenido ambiguo.

Si bien en el dataset se etiqueta como negativa, esta crítica ha sido clasificada por el modelo *RidgeClassifier* como positiva.

No obstante, desde mi perspectiva, esta discrepancia no debe considerarse como un fallo del modelo, sino más bien como una limitación del propio dataset, ya que el contenido de la crítica expresa una valoración general moderadamente positiva, a pesar de incluir observaciones críticas.

De esta manera, algunas expresiones que justifican la predicción del modelo son
"very well produced and directed" con un modificador que intensifica la polaridad ya  en sí positiva del adverbio *well* o "I liked this movie..." con un verbo que expresa agrado. Además, el cierre expresivo con modalidad oracional exclamativa, "congratulations!", refuerza la polaridad positiva.

Con todo, aunque no denotan rechazo, en la reseña también aparecen juicios críticos: "the story... well it needed something else..."; "...script is always superficial"; "..impression that I have seen a movie trailer" que explican la clasificación negativa asignada por el dataset (probablemente en la frontera entre ambas categorías).

En contraposición al ejemplo anterior, la reseña 23262 es un caso de concordancia de etiquetado entre dataset y modelo.

A pesar de que el emisor menciona ciertas objeciones acerca del argumento: "one or two little niggles for me in the story", estas son inmediatamente relativizadas a través de la conjunción adversativa *but*: "but I looked past them and just enjoyed the film for what it was" ya desde el inicio textual, se observa una expresión de preferencia que establece un marco afectivo positivo "One of my favourite films, whenever it is on..." e, incluso, cuando se señala que el argumento no es original, se valora la capacidad de entretenimiento de la película explícitamente: "the story... still felt entertaining"

Finalmente, la puntuación con la que se cierra el texto: “Overall I give it a 7/10”, evita lugar a la confusión clasificatoria binaria.

### 3.6. Interpretación de los coeficientes del modelo lineal: n-gramas frente a características léxicas

Se presenta a continuación el análisis interpretativo de los coeficientes obtenidos por cada modelo, así como una comparación visual de los patrones lingüísticos más influyentes.

La función `analizar_modelo_lineal()` separa las características n-gramas (extraídas mediante `TfidfVectorizer`) de las características léxicas (calculadas a partir de SentiWordNet) y representa visualmente sus pesos más influyentes.

Dado que los modelos analizados utilizan `RidgeClassifier`, se ha adaptado la función para que mapee correctamente los coeficientes y funcione de forma equivalente a como lo haría con `LogisticRegression`.

Para aplicar esta herramienta, se han entrenado y analizado dos modelos previamente definidos:

- `pipe_A`: basado exclusivamente en n-gramas  
- `pipe_B`: que combina n-gramas con características léxicas (`swn_pos_sum`, `swn_neg_sum`)

Esta visualización permite interpretar cómo influye cada tipo de señal textual en la decisión del modelo correspondiente.
"""

def analizar_modelo_lineal(pipeline, top_n=2):

    union = pipeline.named_steps['features']
    model = pipeline.named_steps['clf']


    if not hasattr(model, "coef_"):
        raise ValueError("El modelo no tiene coeficientes interpretables (coef_).")

    feat_names = union.get_feature_names_out()
    coef = model.coef_

    if np.isscalar(coef):
        coef = np.array([coef])

    mask_ng = [fn.startswith('ngrams__') for fn in feat_names]
    mask_lex = [fn.startswith('lexico__') for fn in feat_names]


    df_ng_all = pd.DataFrame({
        'feature': [fn.replace('ngrams__','') for fn, m in zip(feat_names, mask_ng) if m],
        'coef': coef[mask_ng]
    })

    df_ng_pos = df_ng_all.nlargest(top_n, 'coef')
    df_ng_neg = df_ng_all.nsmallest(top_n, 'coef')

    df_lx = pd.DataFrame({
        'feature': [fn.replace('lexico__','') for fn, m in zip(feat_names, mask_lex) if m],
        'coef': coef[mask_lex]
    })
    df_lx = df_lx.reindex(df_lx['coef'].abs().sort_values(ascending=False).index)

    # Visualización
    n_plots = 2 + (1 if not df_lx.empty else 0)

    if not df_lx.empty:
        fig, axes = plt.subplots(n_plots, 1, figsize=(10, 4 * n_plots))
        ax_ng_pos = axes[0]
        ax_ng_neg = axes[1]
        ax_lex = axes[2]
    else:
        fig, axes = plt.subplots(n_plots, 1, figsize=(10, 4 * n_plots))
        ax_ng_pos = axes[0]
        ax_ng_neg = axes[1]


    sns.barplot(x='coef', y='feature', data=df_ng_pos, ax=ax_ng_pos, palette='Greens')
    ax_ng_pos.set_title(f'Top {top_n} n-gramas positivos')

    sns.barplot(x='coef', y='feature', data=df_ng_neg, ax=ax_ng_neg, palette='viridis')
    ax_ng_neg.set_title(f'Top {top_n} n-gramas negativos')


    if not df_lx.empty:
        sns.barplot(x='coef', y='feature', data=df_lx, ax=ax_lex, palette='rocket')
        ax_lex.set_title('Coeficientes léxicos')

    plt.tight_layout()
    plt.show()

    return df_ng_all, df_lx

pipe_A = Pipeline([
    ('features', FeatureUnion([
        ('ngrams', TfidfVectorizer(
            tokenizer=tokenizar,
            ngram_range=(2, 3),
            min_df=3,
            token_pattern=None
        ))
    ])),
    ('clf', RidgeClassifier(alpha=1.0))
])

pipe_B = Pipeline([
    ('features', FeatureUnion([
        ('ngrams', TfidfVectorizer(
            tokenizer=tokenizar,
            ngram_range=(2, 3),
            min_df=3,
            token_pattern=None
        )),
        ('lexico', SentiWordNetScoreExtractor2())
    ])),
    ('clf', RidgeClassifier(alpha=1.0))
])

"""### 3.6. 1. Detección de patrones afectivos mediante n-gramas"""

pipe_A.fit(X_train, y_train)
df_A, _ = analizar_modelo_lineal(pipe_A)

"""En este tipo de modelo, durante el entrenamiento se asigna a cada característica, en este caso, a cada n-grama, un coeficiente que representa su peso en la predicción: si el coeficiente es positivo, la presencia del n-grama aumenta la probabilidad de que la reseña sea clasificada como positiva; si es negativo, la favorece como negativa. Si el valor es cercano a cero, su influencia es prácticamente nula.

Como se puede observar, se han generado dos gráficos: uno con los 15 n-gramas más positivos y otro con los más negativos. El eje vertical muestra los n-gramas más influyentes y el eje horizontal refleja el valor del coeficiente asociado a cada uno.

Entre los n-gramas positivos destacan expresiones con una evidente connotación positiva como *the best*, *a great*, *an excellent*, o *well worth*, cuyos altos coeficientes ejercen una clara influencia en la clasificación positiva de la reseña.

Por lo que respecta a los n-gramas negativos, se observa especialmente el peso ejercido por el superlativo *the worst*, precisamente el antónimo del n-grama positivo más influyente, *the best*. Se establece así una correlación entre la manera en que el lenguaje natural expresa máxima cualidad y cómo el modelo lo detecta, debido a la frecuencia con que estos n-gramas superlativos aparecen en reseñas relacionados, respectivamente, con una u otra polaridad.

Asimismo, expresiones como *the worst*, *waste of*, *not worth*, o *fails to*, aunque con coeficientes menores, tienen magnitudes negativas que reflejan que el modelo capta la polaridad negativa de sentimiento.

Este análisis pone de manifiesto el valor de los n-gramas como unidades representativas en tareas de Análisis de Sentimiento, pues las combinaciones léxicas como *the best* transmiten, ya no solo matices afectivos que los unigramas no reflejan, sino rasgos estructurales de la lengua relacionados con la expresión de lo positivo y negativo.

En definitiva, el modelo lineal basado en `RidgeClassifier` y n-gramas ha demostrado una alta capacidad para identificar patrones lingüísticos relevantes que evidencian que, incluso sin incorporar información semántica externa, un modelo lineal puede capturar con eficacia señales léxicas de tipo afectivo.

### 3.6.2. Efecto de la incorporación de atributos léxicos
"""

pipe_B.fit(X_train, y_train)
df_B, df_B_lex = analizar_modelo_lineal(pipe_B)

"""Al comparar los gráficos del modelo sin lexicón con los del modelo con él se observa cómo en este último el rasgo añadido contribuye a que el clasificador tome decisiones.

De esta manera, el hecho de que los coeficientes de `swn_pos_sum` y `swn_neg_sum`sean positivos y negativos respectivamente, refleja una relación directa entre la polaridad semántica del texto y el valor de los coeficientes que demuestra que el modelo está utilizando correctamente la información léxica proporcionada por SentiWordNet.


En concreto, el valor positivo o negativo del coeficiente asociado a la variable correspondiente (≈| 0.045 |) indica que, por cada unidad adicional de positividad léxica acumulada en una reseña, la puntuación del modelo se incrementa en aproximadamente 0.045 unidades hacia la clase correspondiente.

En cuanto a los coeficientes de los n-gramas, se observan ligeras variaciones en el orden y magnitud con respecto al modelo sin las características adicionales. Por ejemplo, n-gramas como *the best o a great*, si bien mantienen una clara influencia positiva en ambos modelos, intercambian posiciones en la jerarquía. Este hecho se debe a que la incorporación de rasgos adicionales conlleva un reajuste de coeficientes.

Además, se manifiesta que los valores alcanzados por los coeficientes de los n-gramas positivos y negativos son más bajos (aunque sigan siendo elevados), ya que con la característica añadida `swn_pos_sum y swn_neg_sum` (que representan la suma de puntuaciones de positividad y negatividad léxica de las palabras del texto ) el modelo tiene más información semántica.

Sin embargo, a pesar de que el modelo con atributos léxicos incorpora información semántica útil mediante las dos variables, los resultados cuantitativos muestran, como se ha mencionado anteriormente, que el modelo sin estas características ha obtenido un rendimiento ligeramente superior, de lo que se desprende que, en este conjunto de datos, los n-gramas son suficientes para que el modelo clasifique adecuadamente y la señal adicional proporcionada por el léxico parece restar ligeramente su eficacia.

### 3. 7. Generación y análisis de pérdidas por predicción

Dado que con el modelo con el mejor rendimiento, *RidgeClassifier,* no es posible calcular directamente métricas basadas en probabilidades, como la pérdida logarítmica o realizar una interpretación cualitativa con herramientas como LIME, se ha optado por entrenar un modelo basado en *LogisticRegression* con una representación textual muy similar y alto rendimiento según las métricas realizadas (TfidfVectorizer con bigramas).

Con el bloque de código adyacente se pretende  analizar el rendimiento del mencionado modelo de Regresión Logística mediante el cálculo de la pérdida logarítmica por muestra.

Primero, se han obtenido las predicciones (`y_pred`) y las probabilidades de clase para cada muestra del conjunto de test. Luego, se ha calculado la pérdida logarítmica individual para cada reseña con el fin de evaluar la confianza del modelo en su predicción.

Además, para realizar un mejor análisis, las reseñas se han impreso íntegramente
y se han mostrado las diez primeras con mayor pérdida.

Finalmente, se ha generado un gráfico que representa la distribución de la pérdida por clase real (*positive y negative*).
"""

pipe_log = Pipeline([
    ('vect', TfidfVectorizer(
        tokenizer=tokenizar,
        ngram_range=(2, 2),
        token_pattern=None
    )),
    ('clf', LogisticRegression(
        penalty='l2',
        solver='saga',
        C=1.0,
        max_iter=1000,
        random_state=42
    ))
])


pipe_log.fit(X_train, y_train)


y_pred = pipe_log.predict(X_test)
probas = pipe_log.predict_proba(X_test)

true_class_indices = y_test.map({'negative': 0, 'positive': 1}).values


loss = -np.log(probas[np.arange(len(y_test)), true_class_indices])


tabla_analisis = pd.DataFrame({
    "texto": X_test,
    "etiqueta_real": y_test,
    "prediccion": y_pred,
    "probabilidad_predicha": np.max(probas, axis=1),
    "loss": loss
})
tabla_analisis["correcta"] = tabla_analisis["etiqueta_real"] == tabla_analisis["prediccion"]


pd.set_option('display.max_colwidth', None)
muestras_problematicas = tabla_analisis.sort_values("loss", ascending=False).head(10)
print("=== TOP 10 muestras con mayor pérdida logarítmica ===")
display(muestras_problematicas)


plt.figure(figsize=(8, 5))
sns.boxplot(x="etiqueta_real", y="loss", data=tabla_analisis)
plt.title("Distribución de la pérdida logarítmica por categoría de sentimiento")
plt.xlabel("Etiqueta real")
plt.ylabel("Pérdida logarítmica")
plt.tight_layout()
plt.show()

"""En la revisión de algunas muestras con alta pérdida logarítmica se ha detectado que la etiqueta real del conjunto de datos es positiva, aun cuando el contenido textual refleja claramente una reiterada valoración negativa: *bad acting, bad direction, bad sets, bad cinematography, bad sound, bad sex scenes..How dumb is that?*. Es el caso de la reseña 25161.

Este tipo de inconsistencias en el dataset tiene un impacto negativo en el entrenamiento y evaluación del modelo y sirve para justificar tanto su alta confianza en la clasificación negativa 0. 97	como, en consecuencia, la elevada pérdida logarítmica, 3.52.

Otro caso interesante es el de la reseña 3205, etiquetada positivamente por el modelo con una alta probabilidad (0.907), pero con una etiqueta real negativa, por lo que la pérdida logarítmica es significativa (2.375).

En este ejemplo, la reseña presenta una polaridad ambigua que en un etiquetador multiclase podría haberse clasificado como neutral: el texto comienza con expresiones de entusiasmo y oraciones con modalidad exclamativa (“I was so excited and hyped up about watching this film when the promos first came out!"), continúa con una crítica hacia el exceso de personajes y el desnudo en una escena, expresada mediante elementos modales relacionados con una actitud negativa (“Nikhil Advani [...] should not have had many people in this movie”),  (“I found this extremely rude watching this with family!”). Más adelante, sin embargo, se emplean adjetivos valorativos con alta connotación positiva (“The songs are excellent and I think that all of the songs are awesome”) y el cierre denota una evaluación positiva moderada: “Overall, I think it's an alright movie!”. Es, por tanto, probable que el modelo haya clasificado la reseña como positiva por la presencia de términos como *excellent, awesome o alright*, así como por la ambigüedad inherente al lenguaje que implica que, en ocasiones, la clasificación de la polaridad sentimental sea extremadamente compleja, también para las personas y que haya discrepancias entre varios jueces.

En consecuencia, estos ejemplos justifican el uso de la pérdida logarítmica como herramienta de diagnóstico complementaria al análisis cuantitativo para descubrir errores de etiquetado que revelan limitaciones inherentes al dataset, así como la naturaleza subjetiva y ambigua del lenguaje connotativo que dificultan una clasificación categórica precisa, que, será mejorada por modelos basados en Transformers.

#### 3. 7. 1. Breve análisis del gráfico boxplot

El gráfico muestra la distribución de la pérdida logarítmica por clase real.

El eje de abscisas representa la etiqueta real de la reseña (*negative, positive*) y el de ordenadas, el valor de la pérdida logarítmica por muestra.

Dado que la mayoría de las pérdidas son inferiores a 2.5, se deduce que el modelo, o bien acierta frecuentemente, o bien se equivoca con poca confianza.

Los *outliers*, muestran que, aunque pocas, hay etiquetas, como las analizadas anterioremente, en las que, bien el modelo, bien el etiquetador del dataset erró.

### 3. 8. Evaluación e interpretación mediante LIME

LIME *(Local Interpretable Model-agnostic Explanations*) es una herramienta que posibilita comprender las decisiones tomadas por el modelo, ya que resalta las palabras que más influyen en cada predicción.

 Antes de su empleo se han identifiado los casos en los que la predicción no coincide con la etiqueta real para seleccionar así las reseñas más problemáticas.
"""

y_pred = pipe_log.predict(X_test)

errores = np.where(y_pred != y_test)[0]

for idx_pos in errores[:2]:
    print(f"Índice (posición) {idx_pos} - Real: {y_test.iloc[idx_pos]} / Predicho: {y_pred[idx_pos]}")
    print(X_test.iloc[idx_pos])
    print("------")

class_names = list(pipe_log.named_steps['clf'].classes_)
explainer = LimeTextExplainer(class_names=class_names)

idx = [4, 11]

for idx_pos in idx:
    text_instance = X_test.iloc[idx_pos]
    true_label = y_test.iloc[idx_pos]

    exp = explainer.explain_instance(
        text_instance,
        classifier_fn=predict_lime,
        num_features=10,

    )

    print(f"\nExplicación para índice (posición) {idx_pos}")
    print("Texto:", text_instance)
    print("Etiqueta real:", true_label)

    exp.show_in_notebook(text=text_instance)

"""####  3. 8. 1. Instancia con el índice 4

El modelo clasifica la reseña correspondiente al índice 4 como positiva con una probabilidad del 69%, a pesar de que la etiqueta real es negativa y de que el texto contiene expresiones que denotan una polaridad negativa evidente (“poorly realized drama”, “It is a terrible movie”).

LIME refleja cómo la decisión del modelo está determinada por tokens como *best*, con un alto peso positivo (0.08) que pondera más que los aportados por otras voces como *terrible* (0.06).

Este caso evidencia cómo los modelos basados en n-gramas se equivocan al interpretar el tono global del texto cuando este se basa en palabras con polaridades de las dos clases, pues su análisis no es contextual, sino frecuencial.

####  3. 8. 2. Instancia con el índice 11

La crítica cinematográfica 11 es clasificada como negativa con una probabilidad del 63%, frente a la etiqueta del dataset, positiva.

Este ejemplo pone de manifiesto la dificultad, antes señalada, de clasificar ciertas reseñas cuando son ambiguas.

Aunque el clasificador identifica elementos lingüísticos neutros como negativos porque en su aprendizaje estadísticamente se han relacionado con la negatividad (*at, all o guess*), tokens como *great, best, done, well* con una connotación positiva suman menos peso.

Este caso pone de relieve los límites de los enfoques dicotómicos estadísticos ante textos matizados, debido a carencias analíticas contextuales y semánticas que se verán compensadas por los Transformers.

# Segunda parte: Aprendizaje Profundo para el Análisis de Sentimiento con IMDb

En esta sección se desarrollan y evalúan modelos basados en redes neuronales recurrentes (RNNs y LSTMs) y modelos de aprendizaje transferido mediante transformadores, como parte del análisis de sentimiento.

A diferencia del aprendizaje clásico, que requiere diseñar manualmente las variables de entrada del modelo (como n-gramas o características léxicas), los modelos de Aprendizaje Profundo extraen automáticamente representaciones abstractas a partir de los datos, por lo que las tareas de clasificación de Análisis de Sentimiento se facilitan, aunque, como se demostrará, no siempre con mejor rendimiento.

### II. a) RNNs y LSTMs

### II. a) 1. Preprocesamiento textual y preparación de datos para modelos de Aprendizaje Profundo

En primer lugar se han importado las librerías necesarias para procesar las reseñas adecuadamente con el fin de entrenar modelos basados en redes neuronales.

De esta manera, se ha configurado la reproducibilidad a través de semillas aleatorias con `random.seed, np.random.seed y tf.random.set_seed,`.

Además, se ha importado Tensorflow y desde *tensorflow.keras* las herramientas `Tokenizer`, que convierte los textos en secuencias de enteros, y `pad_sequences`, que iguala la longitud de las secuencias mediante relleno.

También se ha cargado `Sequential`, para construir modelos de redes neuronales de forma secuencial, así como las capas necesarias: `Embedding` (para transformar índices de palabras en vectores densos), `SimpleRNN` (red neuronal recurrente simple), LSTM (capa recurrente con memoria a largo plazo), Dense (capa totalmente conectada) y `GlobalAveragePooling1D`, que promedia los estados ocultos de la secuencia y obtiene una representación global del texto antes de la clasificación.

Por último, se han importado `EarlyStopping` para detener el entrenamiento automáticamente cuando la pérdida de validación deja de mejorar,`StandardScaler` para el escalado de atributos y herramientas de HuggingFace Transformers como `AutoTokenizer, AutoModelForSequenceClassification y Trainer,` con el fin de emplear modelos preentrenados.
"""

!pip install --upgrade datasets transformers

# Reproducibilidad
import random
import numpy as np
from numpy import array
import tensorflow as tf
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

# Preprocesamiento de texto
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Modelado
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (
    Embedding, SimpleRNN, Bidirectional, GlobalAveragePooling1D,
    Dense, LSTM, Input, Concatenate
)
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import backend as K

from sklearn.preprocessing import StandardScaler


# Carga de datos y modelos Transformers
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding
)

"""Posteriormente, se ha realizado un mapeo de las etiquetas categóricas: los valores *negative y positive* se han transformado en representaciones numéricas binarias (0 y 1, respectivamente).

Además, se han segmentado los conjuntos de datos en variables independientes y dependientes: las reseñas textuales, *review,* son asignadas como características predictoras (X), mientras que los valores correspondientes a *sentiment* se utilizan como variable objetivo (y).
"""

label_map = {'negative': 0, 'positive': 1}
train_df['sentiment'] = train_df['sentiment'].map(label_map)
valid_df['sentiment'] = valid_df['sentiment'].map(label_map)
test_df['sentiment'] = test_df['sentiment'].map(label_map)

X_train = train_df["review"]
y_train = train_df["sentiment"]

X_valid = valid_df["review"]
y_valid = valid_df["sentiment"]

X_test = test_df["review"]
y_test = test_df["sentiment"]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_valid_seq = tokenizer.texts_to_sequences(X_valid)
X_test_seq = tokenizer.texts_to_sequences(X_test)
X_train_seq

"""Con `tokenizer.word_index` se observa el vocabulario creado a partir del conjunto de entrenamiento. Contiene un diccionario en el que cada palabra única encontrada en los textos se asocia con un índice que es el entero que se empleará para convertir las reseñas en secuencias numéricas."""

tokenizer.word_index

"""Se han definido las variables necesarias para entrenar los modelos: el número de clases a predecir, 2,  y el tamaño del vocabulario. Asimismo, se ha aplicado *padding* a las secuencias de texto para que tengan la misma longitud (maxlen = 120)."""

vocab_size = len(tokenizer.word_index) + 1
maxlen = 120
num_classes = 2

X_train_pad = pad_sequences(X_train_seq, padding='post', maxlen=maxlen)
X_valid_pad = pad_sequences(X_valid_seq, padding='post', maxlen=maxlen)
X_test_pad = pad_sequences(X_test_seq, padding='post', maxlen=maxlen)

X_train_pad

"""Además, se ha definido una función que construye la matriz de *embeddings* a partir de los vectores preentrenados GloVe (*Global Vectors for Word Representation*) con el objetivo de integrarla las configuraciones de los modelos, pues en otras prácticas se ha demostrado que su incorporación mejora el rendimiento de los modelos.

También se ha incluido una función auxiliar para cargar automáticamente la versión deseada de GloVe, según su dimensión.
"""

# Función que carga los vectores GloVe
def create_embedding_matrix(filepath, word_index, embedding_dim):
    vocab_size = len(word_index) + 1
    embedding_matrix = np.zeros((vocab_size, embedding_dim))
    with open(filepath, encoding='utf-8') as f:
        for line in f:
            word, *vector = line.split()
            if word in word_index:
                idx = word_index[word]
                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]
    return embedding_matrix

# Carga automática por dimensión
def cargar_embedding_matrix(embedding_dim):
    ruta_glove = f'/content/drive/MyDrive/Máster/glove.6B.{embedding_dim}d.txt'
    return create_embedding_matrix(ruta_glove, tokenizer.word_index, embedding_dim)

"""### II. a. 2. Entrenamiento y evaluación de arquitecturas RNNs mediante búsqueda de hiperparámetros en IMDb: 72 combinaciones

Para este experimento se ha diseñado una búsqueda de hiperparámetros sobre una red RNN aplicada al corpus IMDb.

La configuración ha partido de valores previamente identificados como óptimos en una práctica anterior realizada sobre el corpus de Amazon de clasificación multiclase.

A pesar de que en este caso el problema es de clasificación binaria, en general, se ha decidido mantener los mismos rangos de mejores hiperparámetros  porque esos valores habían mostrado un buen equilibrio entre rendimiento y eficiencia computacional en el contexto anterior.

Concretamente, se han probado 72 combinaciones entre `embedding_dim` = [100, 200], `rnn_units` `código insertado`  = [128, 256], `epoch`s = [5, 10, 50] y patience = `[5, 15], `junto con tres arquitecturas distintas:` simple, stacked y bidirectional`, todas ellas combinaciones entrenadas mediante la función de pérdida `binary_crossentropy` y optimizador Adam y con la aplicación de `EarlyStopping` para evitar el sobreentrenamiento.

Además, se ha añadido una capa oculta Dense(64, activation='relu') antes de la capa de salida Dense(1, activation='sigmoid'). Esta capa ha incorporado una no linealidad adicional entre la representación final de la secuencia y la predicción binaria, que, como se demostrará posteriormente ha implicado mejores resultados.

La evaluación se ha realizado con las métricas *Accuracy, Macro F1 y Hamming Loss,* y los resultados se han ordenado para seleccionar las mejores configuraciones.
"""

# 1. Hiperparámetros a probar
embedding_dims   = [100, 200]
rnn_units_list   = [128, 256]
epochs_list      = [5, 10, 50]
patience_list    = [5, 15]
architectures    = ['simple', 'stacked', 'bidirectional']

# 2. Guardar resultados
resultados = []

# 3. Bucle de combinaciones
for embedding_dim in embedding_dims:
    ruta_glove = f'/content/drive/MyDrive/Máster/glove.6B.{embedding_dim}d.txt'
    embedding_matrix = create_embedding_matrix(ruta_glove, tokenizer.word_index, embedding_dim)

    for rnn_units in rnn_units_list:
        for epochs in epochs_list:
            for patience in patience_list:
                for arch in architectures:

                    model = Sequential()
                    model.add(Embedding(input_dim=vocab_size,
                                        output_dim=embedding_dim,
                                        weights=[embedding_matrix],
                                        trainable=True,
                                        input_length=maxlen))

                    if arch == 'simple':
                        model.add(SimpleRNN(rnn_units, return_sequences=True))
                    elif arch == 'stacked':
                        model.add(SimpleRNN(rnn_units, return_sequences=True))
                        model.add(SimpleRNN(rnn_units // 2, return_sequences=True))
                    elif arch == 'bidirectional':
                        model.add(Bidirectional(SimpleRNN(rnn_units, return_sequences=True)))

                    model.add(GlobalAveragePooling1D())
                    model.add(Dense(64, activation='relu'))
                    model.add(Dense(1, activation='sigmoid'))

                    model.compile(optimizer='adam',
                                  loss='binary_crossentropy',
                                  metrics=['accuracy'])

                    early_stop = EarlyStopping(monitor='val_loss',
                                               patience=patience,
                                               restore_best_weights=True)

                    history = model.fit(X_train_pad, y_train,
                                        epochs=epochs,
                                        batch_size=512,
                                        validation_data=(X_valid_pad, y_valid),
                                        callbacks=[early_stop],
                                        verbose=0)

                    y_pred = (model.predict(X_test_pad) > 0.5).astype(int).flatten()

                    resultados.append({
                        "Embedding_Dim": embedding_dim,
                        "RNN_Units": rnn_units,
                        "Epochs": epochs,
                        "Patience": patience,
                        "Architecture": arch,
                        "Macro_F1": f1_score(y_test, y_pred, average='macro', zero_division=0),
                        "Accuracy": accuracy_score(y_test, y_pred),
                        "Hamming_Loss": hamming_loss(y_test, y_pred)
                    })

# 4. Resultados ordenados por Macro F1
df_resultados = pd.DataFrame(resultados)
df_ordenado = df_resultados.sort_values(by="Macro_F1", ascending=False)

print("\nMejores configuraciones en IMDb:")
print(df_ordenado.head())


# 5. Guardar resultados en un archivo CSV
df_ordenado.to_csv("resultados_imdb_grid.csv", index=False)
print("\nResultados guardados en 'resultados_imdb_grid.csv'")

"""#### II. a. 2. 1.  Análisis de los resultados


Se observan los siguientes patrones entre las configuraciones con mejor rendimiento:

1) Una arquitectura *Bidirectional RNN*, que muestra que captar el contexto en ambas direcciones mejora el rendimiento en el dataset de IMDb.

2) Los valores más altos de Macro-F1 se alcanzan con `embedding_dim` = 200.

3) Los mejores tiempos de entrenamiento son cortos (`epochs` = 5), de lo que se deduce que el modelo aprende rápidamente y que el `EarlyStopping` es efectivo.

El mejor modelo obtenido ha sido una red Bidirectional RNN con 256 unidades, embedding de 200 dimensiones, 5 épocas y paciencia de 5, con una puntuación de Macro F1 =0.883162

### II. a. 3. Entrenamiento y evaluación de modelos LSTM mediante búsqueda de hiperparámetros en IMDb: 108 combinaciones

Dado que las *Long Short-Term Memory networks* (LSTM) son una variante avanzada de las RNNs, con una arquitectura interna más compleja basada en mecanismos de compuertas (*gate mechanisms*) que retienen o descartan información de manera controlada, pueden ofrecer mejores resultados en secuencias largas que las RNN simples.

Por este motivo, en esta sección se ha realizado una búsqueda de hiperparámetros sobre un total de 108 combinaciones, a partir de los siguientes valores:  
`embedding_dim = [100, 200, 300]`,  
`LSTM_units = [64, 128, 256]`,  
`epochs = [10, 20, 50, 100]` y  
`patience = [5, 10, 15]`.

Como en el caso anterior, estos modelos han sido entrenados con pesos preentrenados de GloVe, mediante la función de pérdida `binary_crossentropy` y el optimizador `Adam`. Además, se ha añadido una capa intermedia `Dense(64, activation='relu')` antes de la salida binaria `Dense(1, activation='sigmoid')`.

Cada modelo se ha entrenado aplicando `EarlyStopping`, ajustado al valor de paciencia correspondiente, y con una arquitectura LSTM configurada con `return_sequences=True`, seguida de una capa `GlobalAveragePooling1D()` para resumir la información temporal de la secuencia.
"""

# 1. Hiperparámetros
embedding_dims = [100, 200, 300]
lstm_units_list = [64, 128, 256]
epochs_list = [10, 20, 50, 100]
patience_list = [5, 10, 15]

#
resultados = []

# 2. Bucle de combinaciones
for embedding_dim in embedding_dims:
    ruta_glove = f'/content/drive/MyDrive/Máster/glove.6B.{embedding_dim}d.txt'
    embedding_matrix = create_embedding_matrix(ruta_glove, tokenizer.word_index, embedding_dim)

    for lstm_units in lstm_units_list:
        for epochs in epochs_list:
            for patience in patience_list:

                model = Sequential()
                model.add(Embedding(input_dim=vocab_size,
                                    output_dim=embedding_dim,
                                    weights=[embedding_matrix],
                                    trainable=True,
                                    input_length=maxlen))
                model.add(LSTM(lstm_units, return_sequences=True))
                model.add(GlobalAveragePooling1D())
                model.add(Dense(64, activation='relu'))
                model.add(Dense(1, activation='sigmoid'))

                model.compile(optimizer='adam',
                              loss='binary_crossentropy',
                              metrics=['accuracy'])

                early_stop = EarlyStopping(monitor='val_loss',
                                           patience=patience,
                                           restore_best_weights=True)

                model.fit(X_train_pad, y_train,
                          epochs=epochs,
                          batch_size=512,
                          validation_data=(X_valid_pad, y_valid),
                          callbacks=[early_stop],
                          verbose=0)

                y_pred = (model.predict(X_test_pad) > 0.5).astype(int).flatten()

                resultados.append({
                    "Embedding_Dim": embedding_dim,
                    "LSTM_Units": lstm_units,
                    "Epochs": epochs,
                    "Patience": patience,
                    "Macro_F1": f1_score(y_test, y_pred, average='macro', zero_division=0),
                    "Accuracy": accuracy_score(y_test, y_pred),
                    "Hamming_Loss": hamming_loss(y_test, y_pred)
                })

# 3. Crear DataFrame y mostrar ordenado por Macro F1
df_resultados = pd.DataFrame(resultados)
df_ordenado = df_resultados.sort_values(by="Macro_F1", ascending=False)

print("\nMejores configuraciones LSTM en IMDb:")
print(df_ordenado.head())

# 4. Guardar resultados en CSV
df_ordenado.to_csv("resultados_imdb_lstm_relu.csv", index=False)
print("\nResultados guardados en 'resultados_imdb_lstm_relu.csv'")

"""#### II. a) 3.1. Análisis de los resultados LSTM

Llama la atención que las mejores configuraciones emplean LSTM con 256 unidades, hecho que refleja que configuraciones menores no son suficientes para modelar las dependencias contextuales del corpus IMDb.

En cuanto a la dimensión de los embeddings, los mejores resultados se distribuyen entre `embedding_dim` = 100, 200 y 300, aunque el valor más alto de Macro-F1 corresponde a `embedding_dim` = 100, seguido de 200 y 300.

Respecto a `epochs y patience`, se observa que las combinaciones más efectivas se sitúan en epochs = 20 o 50 con patience = 5 o 10. Esto implica que el modelo alcanza su mejor rendimiento relativamente rápido.

### II. a). 4. Comparativa entre el mejor modelo RNN y el mejor LSTM


| Característica          | Bidirectional SimpleRNN | LSTM (unidireccional) |
|-------------------------|--------------------------|------------------------|
| Embedding_Dim           | 200                      | 100                    |
| Unidades RNN/LSTM       | 256                      | 256                    |
| Epochs                  | 5                        | 20                     |
| Patience                | 5                        | 5                      |
| Arquitectura            | Bidirectional SimpleRNN  | LSTM unidireccional    |
| **Macro F1**            | **0.883162**             | **0.883198**           |
| Accuracy                | 0.8832                   | 0.8832                 |
| Hamming Loss            | 0.1168                   | 0.1168                 |



Ambos modelos, RNN y LSTM han alcanzado rendimientos muy similares en las tres métricas clave, la diferencia en Macro-F1 es muy leve y en *Accuracy* y función de pérdida es idéntica. Sin embargo, debe tenerse en cuenta que el modelo *Bidirectional RNN* logra ese rendimiento con menos épocas, por lo que es más eficiente.

De este modo, parece que el carácter bidireccional de RNN compensa la simplicidad del SimpleRNN, mientras que el LSTM unidireccional necesita más entrenamiento para igualarlo.

### II. a) 5. Experimentos con variantes de la mejor versión obtenida LSTM (100 dimensiones,256 unidades, 20 épocas, paciencia 5)

### II a) 5. 1. Sin la capa de activación ReLU (*Rectified Linear Unit*)

En esta variante del experimento, se ha evaluado el impacto de eliminar la capa previa a la salida binaria con activación ReLU para analizar si su ausencia afecta negativamente al rendimiento del modelo LSTM.
"""

# 1. Hiperparámetros específicos
embedding_dim = 100
lstm_units = 256
epochs = 20
patience = 5

ruta_glove = f'/content/drive/MyDrive/Máster/glove.6B.{embedding_dim}d.txt'
embedding_matrix = create_embedding_matrix(ruta_glove, tokenizer.word_index, embedding_dim)

# 2 Definir modelo sin capa relu
model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=embedding_dim,
                    weights=[embedding_matrix],
                    trainable=True))
model.add(LSTM(lstm_units, return_sequences=True))
model.add(GlobalAveragePooling1D())
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# 5. Entrenamiento
early_stop = EarlyStopping(monitor='val_loss',
                           patience=patience,
                           restore_best_weights=True)

model.fit(X_train_pad, y_train,
          epochs=epochs,
          batch_size=512,
          validation_data=(X_valid_pad, y_valid),
          callbacks=[early_stop],
          verbose=1)

# 6. Evaluación
y_pred = (model.predict(X_test_pad) > 0.5).astype(int).flatten()

macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)
accuracy = accuracy_score(y_test, y_pred)
hamming = hamming_loss(y_test, y_pred)

# 7. Mostrar resultados
print("\nResultados modelo sin relu:")
print("Macro F1:", macro_f1)
print("Accuracy:", accuracy)
print("Hamming Loss:", hamming)

"""| Característica        | LSTM con ReLU       | LSTM sin ReLU     |
|-----------------------|---------------------|-------------------|
| Macro F1              | **0.883198**        | **0.864318**          |
| Accuracy              | **0.8832**          | **0.8648**            |
| Hamming Loss          | **0.1168**          | **0.1352**            |
| Epochs                | 20                  | 20                |
| LSTM Units            | 256                 | 256               |
| Patience              | 5                   | 5                 |
| Embedding Dim         | 100                 | 100               |



Los resultados muestran una disminución en el rendimiento al eliminar la capa ReLU.

### II. a) 5. 2. Modelo LSTM multientrada con mecanismo de atención y características léxicas basadas en SentiWordNet


Para este experimento se ha partido del mejor modelo LSTM y se ha construido otro multientrada con atención adaptada al problema de clasificación binaria sobre IMDb con el fin de evaluar si la combinación de una capa de atención personalizada y la incorporación de características léxicas externas mejora el rendimiento del mejor modelo.

Simultáneamente, se han añadido dos características léxicas por instancia, extraídas mediante el uso de SentiWordNet: el promedio de las puntuaciones positivas y negativas de las palabras del texto, previamente lematizadas y etiquetadas gramaticalmente.

La combinación resultante se ha pasado por una capa densa con activación ReLU, seguida de una salida binaria con activación sigmoide.

De esta manera, se ha pretendido enriquecer la capacidad de representación del modelo.
"""

extractor = SentiWordNetScoreExtractor2()

X_train_lex = extractor.transform(X_train)
X_valid_lex = extractor.transform(X_valid)
X_test_lex  = extractor.transform(X_test)

# 1. Capa de atención
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, return_attention=False, **kwargs):
        self.return_attention = return_attention
        super().__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='att_weight',
                                 shape=(input_shape[-1], input_shape[-1]),
                                 initializer='glorot_uniform',
                                 trainable=True)
        self.b = self.add_weight(name='att_bias',
                                 shape=(input_shape[-1],),
                                 initializer='zeros',
                                 trainable=True)
        self.u = self.add_weight(name='context_vector',
                                 shape=(input_shape[-1],),
                                 initializer='glorot_uniform',
                                 trainable=True)
        super().build(input_shape)

    def call(self, inputs, **kwargs):
        uit = K.tanh(K.dot(inputs, self.W) + self.b)
        ait = K.dot(uit, K.expand_dims(self.u))
        ait = K.squeeze(ait, -1)
        a = K.softmax(ait)
        a_expanded = K.expand_dims(a)
        weighted_input = inputs * a_expanded
        context = K.sum(weighted_input, axis=1)
        return context

# 2. Hiperparámetros
embedding_dim = 100
lstm_units = 256
patience = 5
epochs = 20

# 3. Cargar embeddings preentrenados GloVe
ruta_glove = f'/content/drive/MyDrive/Máster/glove.6B.{embedding_dim}d.txt'
embedding_matrix = create_embedding_matrix(ruta_glove, tokenizer.word_index, embedding_dim)

# 4. Modelo multientrada con atención y embeddings preentrenados
text_input = Input(shape=(120,), name='text_input')
embedding = Embedding(input_dim=vocab_size,
                      output_dim=embedding_dim,
                      weights=[embedding_matrix],
                      trainable=True)(text_input)

lstm_out = LSTM(lstm_units, return_sequences=True)(embedding)
att_output = AttentionLayer()(lstm_out)

lex_input = Input(shape=(2,), name='lex_input')
combined = Concatenate()([att_output, lex_input])
dense = Dense(64, activation='relu')(combined)
output = Dense(1, activation='sigmoid')(dense)

model = Model(inputs=[text_input, lex_input], outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

# 5. Entrenamiento
early_stop = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)

model.fit([X_train_pad, X_train_lex],
          y_train,
          validation_data=([X_valid_pad, X_valid_lex], y_valid),
          epochs=epochs,
          batch_size=512,
          callbacks=[early_stop],
          verbose=1)

# 6. Evaluación
y_pred = (model.predict([X_test_pad, X_test_lex]) > 0.5).astype(int).flatten()

from sklearn.metrics import f1_score, accuracy_score, hamming_loss
print("\nResultados modelo MEJORADO con atención y embeddings preentrenados:")
print("Macro F1:", f1_score(y_test, y_pred, average='macro', zero_division=0))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Hamming Loss:", hamming_loss(y_test, y_pred))

"""| Característica          | LSTM (256-20-5)        | LSTM + Atención + Léxico |
|-------------------------|------------------------|---------------------------|
| Embedding_Dim           | 100                    | 100                       |
| LSTM Units              | 256                    | 256                       |
| Epochs                  | 20                     | 20                        |
| Patience                | 5                      | 5                         |
| Entradas adicionales    | No                     | Sí (SentiWordNet)         |
| Mecanismo de atención   | No                     | Sí                        |
| **Macro F1**            | **0.883198**           | **0.873060**              |
| **Accuracy**            | **0.8832**             | **0.8732**                |
| **Hamming Loss**        | **0.1168**             | **0.1268**                |




A pesar de que el modelo incorpora atención y datos léxicos externos, no supera en rendimiento al LSTM puro, probablemente porque el modelo base capta suficientemente bien la información contextual del corpus IMDb.

### II. a) 5. 3.  Evaluación de modelo LSTM multientrada con características léxicas (sin atención)

Finalmente, en este experimento se ha configurado un modelo LSTM multientrada sin capa de atención para observar, si la combinación del texto y las características léxicas, aumentan el rendimiento del modelo sin necesidad de mecanismos de atención.
"""

# 1. Hiperparámetros
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 100
lstm_units = 256
epochs = 20
patience = 5

# 2. Cargar embedding GloVe
ruta_glove = f'/content/drive/MyDrive/Máster/glove.6B.{embedding_dim}d.txt'
embedding_matrix = create_embedding_matrix(ruta_glove, tokenizer.word_index, embedding_dim)

# 3. Modelo multientrada sin atención, con GloVe
text_input = Input(shape=(120,), name='text_input')
embedding = Embedding(input_dim=vocab_size,
                      output_dim=embedding_dim,
                      weights=[embedding_matrix],
                      trainable=True)(text_input)

lstm_out = LSTM(lstm_units, return_sequences=True)(embedding)
pooled = GlobalAveragePooling1D()(lstm_out)

lex_input = Input(shape=(2,), name='lex_input')
combined = Concatenate()([pooled, lex_input])

dense = Dense(64, activation='relu')(combined)
output = Dense(1, activation='sigmoid')(dense)

model = Model(inputs=[text_input, lex_input], outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

# 4. Entrenamiento
early_stop = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)
model.fit([X_train_pad, X_train_lex],
          y_train,
          validation_data=([X_valid_pad, X_valid_lex], y_valid),
          epochs=epochs,
          batch_size=512,
          callbacks=[early_stop],
          verbose=1)

# 5. Evaluación
y_pred = (model.predict([X_test_pad, X_test_lex]) > 0.5).astype(int).flatten()

from sklearn.metrics import f1_score, accuracy_score, hamming_loss
print("\nResultados modelo LSTM + léxico + GloVe (sin atención):")
print("Macro F1:", f1_score(y_test, y_pred, average='macro', zero_division=0))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Hamming Loss:", hamming_loss(y_test, y_pred))

"""| Característica            | LSTM (256-20-5)        | LSTM + Léxico (sin atención) |
|---------------------------|------------------------|-------------------------------|
| Embedding_Dim             | 100                    | 100                           |
| LSTM Units                | 256                    | 256                           |
| Epochs                    | 20                     | 20                            |
| Patience                  | 5                      | 5                             |
| Entradas adicionales      | No                     | Sí (SentiWordNet)             |
| Mecanismo de atención     | No                     | No                            |
| **Macro F1**              | **0.883198**           | **0.872400**                  |
| **Accuracy**              | **0.8832**             | **0.8724**                    |
| **Hamming Loss**          | **0.1168**             | **0.1276**                    |

El modelo multientrada con características léxicas tampoco mejora el rendimiento con respecto al LSTM base.

### II. a) 5. 4. Evaluación del mejor modelo basado en una arquitectura LSTM bidireccional con capa ReLU y *embeddings* preentrenados
"""

# 1. Hiperparámetros específicos
embedding_dim = 100
lstm_units = 256
epochs = 20
patience = 5

# 2. Cargar embedding
ruta_glove = f'/content/drive/MyDrive/Máster/glove.6B.{embedding_dim}d.txt'
embedding_matrix = create_embedding_matrix(ruta_glove, tokenizer.word_index, embedding_dim)

# 3. Definir modelo con LSTM bidireccional + capa ReLU
model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=embedding_dim,
                    weights=[embedding_matrix],
                    trainable=True))
model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))  # LSTM bidireccional
model.add(GlobalAveragePooling1D())
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# 4. Entrenamiento con EarlyStopping
early_stop = EarlyStopping(monitor='val_loss',
                           patience=patience,
                           restore_best_weights=True)

history = model.fit(X_train_pad, y_train,
                    epochs=epochs,
                    batch_size=512,
                    validation_data=(X_valid_pad, y_valid),
                    callbacks=[early_stop],
                    shuffle=True,
                    verbose=1)

# 5. Evaluación
y_pred = (model.predict(X_test_pad) > 0.5).astype(int).flatten()

macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)
accuracy = accuracy_score(y_test, y_pred)
hamming = hamming_loss(y_test, y_pred)

# 8. Mostrar resultados
print("\nRESULTADOS EN EL CONJUNTO DE TEST DEL MODELO LSTM (bidireccional, con ReLU):")
print(f"Macro F1 Score en test: {macro_f1:.4f}")
print(f"Accuracy en test: {accuracy:.4f}")
print(f"Hamming Loss en test: {hamming:.4f}")

"""| Característica            | LSTM (256-20-5)        | LSTM Bidireccional + ReLU |
|---------------------------|------------------------|----------------------------|
| Embedding_Dim             | 100                    | 100                        |
| LSTM Units                | 256                    | 256                        |
| Epochs                    | 20                     | 20                         |
| Patience                  | 5                      | 5                          |
| Bidireccionalidad         | No                     | Sí                         |
| Activación ReLU intermedia| No                     | Sí                         |
| **Macro F1**              | **0.883198**           | **0.8801**                 |
| **Accuracy**              | **0.8832**             | **0.8802**                 |
| **Hamming Loss**          | **0.1168**             | **0.1198**                 |




El modelo bidireccional con ReLU ofrece un rendimiento muy similar al LSTM base, con una ligera caída en las métricas.

### II. a) 5. 5.  Comparativa de modelos LSTM con diferentes configuraciones

| Característica / Métrica           | LSTM unidireccional | LSTM bidireccional | LSTM + Atención + Léxico | LSTM + Léxico (sin atención) | LSTM sin ReLU |
|------------------------------------|----------------------|---------------------|---------------------------|-------------------------------|----------------|
| Atención                           | No                   | No                  | Sí                        | No                            | No             |
| Léxico (SentiWordNet)              | No                   | No                  | Sí                        | Sí                            | No             |
| GloVe                              | Sí                   | Sí                  | Sí                        | Sí                            | Sí             |
| ReLU                               | Sí                   | Sí                  | Sí                        | Sí                            | No             |
| Bidireccionalidad                  | No                   | Sí                  | No                        | No                            | No             |
| **Macro F1**                       | **0.8832**           | **0.8801**          | **0.8731**                    | **0.8724**                    | **0.8643**     |
| **Accuracy**                       | **0.8832**           | **0.8802**          | **0.8732**                    | **0.8724**                    | **0.8648**     |
| **Hamming Loss**                   | **0.1168**           | **0.1198**          | **0.1268**                    | **0.1276**                    | **0.1352**     |


Se observa que ninguna de las variantes evaluadas supera al mejor modelo LSTM base unidireccional con *embeddings* GloVe y capa de activación ReLU antes de la salida.

Se concluye que, debido a la calidad del dataset para esta tarea de clasificación binaria sobre reseñas de IMDb, una arquitectura simple y optimizada es más efectiva que configuraciones más complejas.

### II a) 6. Análisis cualitativo: predicciones de sentimiento del mejor modelo: LSTM unidireccional con *embeddings* GloVe y capa de activación ReLU antes de la salida

Tras entrenarse nuevamente el modelo, se han comparado las predicciones por él realizadas sobre el conjunto de prueba con las etiquetas reales.
"""

embedding_dim = 100
lstm_units = 256
epochs = 20
patience = 5


ruta_glove = f'/content/drive/MyDrive/Máster/glove.6B.{embedding_dim}d.txt'
embedding_matrix = create_embedding_matrix(ruta_glove, tokenizer.word_index, embedding_dim)

model = Sequential()
model.add(Embedding(input_dim=vocab_size,
                    output_dim=embedding_dim,
                    weights=[embedding_matrix],
                    trainable=True))
model.add(LSTM(lstm_units, return_sequences=True))
model.add(GlobalAveragePooling1D())
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

early_stop = EarlyStopping(monitor='val_loss',
                           patience=patience,
                           restore_best_weights=True)

history = model.fit(X_train_pad, y_train,
                    epochs=epochs,
                    batch_size=512,
                    validation_data=(X_valid_pad, y_valid),
                    callbacks=[early_stop],
                    shuffle=True,
                    verbose=1)

y_pred = (model.predict(X_test_pad) > 0.5).astype(int).flatten()
s
df_test = pd.DataFrame({
    "review": X_test,
    "sentiment": y_test,
    "pred_lstm": y_pred
})

df_test["acierto_lstm"] = df_test["pred_lstm"] == df_test["sentiment"]

print("\nEjemplos clasificados incorrectamente por LSTM:")
display(df_test[df_test["acierto_lstm"] == False][["review", "sentiment", "pred_lstm"]].sample(2, random_state=42))

print("\nEjemplos clasificados correctamente por LSTM:")
display(df_test[df_test["acierto_lstm"] == True][["review", "sentiment", "pred_lstm"]].sample(2, random_state=42))

"""### II a) 6.1. Análisis cualitativo de las reseñas 25781 y 39567

El mejor modelo LSTM ha clasificado incorrectamente como *positiva* la reseña 25781, claramente negativa.

Su contenido es una sarcástica y despectiva crítica hacia numerosos aspectos técnicos, de edición e interpretación de la película *Savage Instinct*: "the editing alone is so jumbled you'll think it was assembled by a team of trained (poorly) monkeys, traveling across unpaved canyon road in the back of a jeep, blindfolded and drunk"; "Acting? I can't call anything I saw here acting. Reciting? Hmmm. Can't call it that either. Failing? That works", e, incluso, su emisor la propone como ejemplo de lo que no se debe hacer en clases de cine: "Savage Instinct should be shown in ALL film classes. It is the perfect template for how to not make a movie" y termina con una frase irónica: "Fun, in a strictly masochistic sort of way. Watch it... if you dare."

Son, probablemente, el sarcasmo y la ironía presentes en la reseña los que pueden haber motivado la errada predicción del modelo. De este modo, aunque la reseña contiene términos asociados literalmente a valoraciones positivas como *fun* o *Hilarious*, al estar enunciados irónicamente, invierten su sentido.

Precisamente, estos recursos del lenguaje natural, caracterizados por el desplazamiento de la literalidad, constituyen una de las principales dificultades ante las que se enfrentan ciertos modelos de aprendizaje automático basados en arquitecturas de redes neuronales anteriores a los transformadores, como la aruqitectura LSTM.

En contraposición, la etiqueta *negative* predicha por el modelo de la reseña 39567 es correcta y una de las razones de ello es el uso de un lenguaje claramente literal.

De esta manera, expresiones como: “What it needed was a different director...”, así como el uso del adjetivo con connotación negativa *unbelievable*, intensificado por el modificador *completely*, en referencia a la interpretación de la protagonista: “She was completely unbelievable” no permiten la ambigüedad.

En este sentido, la oración subjetiva en primera persona que expresa un sentimiento de incomodidad en grado elevado, reforzado por un intensificador adverbial: “I'm now beginning to be profoundly embarrassed for everyone involved in this enterprise”, así como el cierre de la crítica, con locuciones adverbiales que, a través de una adjetivación ponderativa,*in a big way*, refuerzan el tono negativo de la reseña en “If you enjoy watching movies that miss their mark in a big way, then watch this one” facilitan la tarea clasificatoria del modelo LSTM.

### II a) 7. Evaluación e interpretación del mejor modelo LSTM mediante LIME

Para comprender las causas por las que el mejor modelo ha asignado etiquetas incorrectas en determinadas predicciones, se han seleccionado ejemplos del conjunto de prueba en los que ha cometido errores de predicción y, posteriormente, se han analizado con LIME.
"""

y_pred = (model.predict(X_test_pad) > 0.5).astype(int).flatten()

errores = np.where(y_pred != y_test)[0]

for idx_pos in errores[:5]:

    print(f"Índice (posición) {idx_pos} - Real: {y_test.iloc[idx_pos]} / Predicho: {y_pred[idx_pos]}")
    print(test_df['review'].iloc[idx_pos])
    print("------")

class_names = ['negative', 'positive']

def predict_lime(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=120, padding='post')
    probs = model.predict(padded)

    return np.hstack([1 - probs, probs])


explainer = LimeTextExplainer(class_names=class_names)

X_test_raw = test_df['review']
y_test_raw = test_df['sentiment']

indices = [3,11]

for idx_pos in indices:

    text_instance = X_test_raw.iloc[idx_pos]
    true_label = y_test.iloc[idx_pos]

    exp = explainer.explain_instance(
        text_instance,
        classifier_fn=predict_lime,
        num_features=10,

    )

    print(f"\n Explicación para índice (posición) {idx_pos}")
    print("Texto:", text_instance)

    print("Etiqueta real:", true_label)
    exp.show_in_notebook(text=text_instance)

"""### II. a) 7.1. Instancia con el índice 3

A pesar de que la etiqueta real de la instancia con índice 3 es negativa, el mejor modelo LSTM la ha considerado incorrectamente  positiva, con una probabilidad del 52%.

Este error resulta relevante porque se trata de una reseña que expresa una dura crítica de la película *Scoop* de Woody Allen, a través de un vocabulario explícitamente desfavorable, con juicios negativos sobre el guion, los personajes y sus actuaciones desde el inicio de la reseña: "Scoop is so bad you'll think "Annie Hall" was a fluke" o "..the movie completely lacks charm, or humor, or atmosphere. It's an amazingly leaden, amateurish effort for someone who has made even one previous film, never mind dozens".

Para explicar este error LIME muestra cómo el modelo se ha visto confundido por antropónimos como *Jackman y Johansson*, así como por *stopwords* como *is y the*, es decir por palabras semánticamente neutras, que , sin embargo, han sido ponderadas como positivas.

También positivas han sido consideradas palabras como *well* en *as well* o *charming*, en referencia a la caracterización de un personaje, es decir que el modelo está realizando un análisis de las palabras sin contextualizarlas.

Es pues, esta atribución ponderativa positiva a elementos carentes de ella, la que conlleva que la ponderación de tokens como *bad *o *nothing* sea menor.

En definitiva, con estos ejemplos se observa cómo el modelo no está contextualizando adecuadamente los tokens, sino que los está interpretando aisladamente.

### II. a) 7.2. Instancia con el índice 11

Como se puede ver en la ilustración, el modelo ha tenido problemas para la clasificación de esta reseña que etiqueta como negativa con una probabilidad del 51%, frente a la real positiva, a la que otorga una probabilidad del 0.49%.

Este error de clasificación se debe a la mayor ponderación total de tokens con connotación negativa como *bad, cheesy, guess y premise* que supera el cómputo del peso de las voces con polaridad positiva como *great, well, moments y best*,aunque tan solo en dos décimas.

Este caso evidencia una limitación común en ciertos modelos de Análisis de Sentimiento: la dificultad para interpretar el tono general cuando los indicadores positivos y negativos están equilibrados.

Además, el hecho de que una etiqueta de salto de línea HTML, *br* haya sido incluida en el análisis indica un problema en el preprocesamiento del texto, que puede haber introducido ruido y , por ello, afectado a la predicción del modelo.

Una vez más, para mejorar la clasificación en casos similares, se propone la incorporación de mecanismos de comprensión semántica más sofisticados que capten la intención general del texto, más allá del conteo de palabras aisladas.

### II. a) 8. Conclusión

| Vectorizador        | Procesamiento  | N-grama | Modelo                      | Macro F1  | Accuracy | Subset Accuracy | Hamming Loss |
|---------------------|----------------|---------|------------------------------|-----------|----------|------------------|---------------|
| TfidfVectorizer     | Tokenización   | (2, 3)  | RidgeClassifier              | **0.9180** | 0.9180   | 0.9180           | 0.0820        |
| GloVe Embeddings    | Secuencial     | —       | LSTM Unidireccional + ReLU  | **0.8832** | 0.8832   | 0.8832           | 0.1168        |



A pesar de los avances obtenidos con modelos basados en redes neuronales como LSTM con *embeddings* GloVe con respecto a los modelos basados en reglas heurísticas, el mejor modelo de Regresión Logística, vectorizado con representaciones TF-IDF con bigramas y trigramas ha demostrado un rendimiento superior en esta tarea específica de clasificación binaria de reseñas de IMDb.

Probablemente, el hecho de que se hayan empleado n-gramas en los modelos de Regresión Logística haya tenido por consecuencia mayor capacitación para aprehender expresiones léxicas compuestas que no se representan explícitamente en el modelo LSTM, ya que este capta relaciones contextuales de forma implícita a través de patrones.

Se concluye, por tanto, que, para esta tarea y este género textual, una representación basada en n-gramas resulta más eficaz que una arquitectura secuencial profunda.

## II. b) Aprendizaje transferido con transformadores

### II. b)1. Preparación del conjunto de datos y tokenización

Tras instalarse las librerías transformers y datasets y convertir los subconjuntos *train, valid y test* al formato DatasetDict mediante la clase `Dataset.from_pandas(`), se han tokenizado los textos mediante el tokenizador correspondiente al modelo seleccionado, `roberta-base`, con la configuración `padding='max_length', truncation=True y `max_length=512.

Asimismo, se ha verificado su correcto procesamiento.
"""

dataset = DatasetDict({
    "train": Dataset.from_pandas(train_df.reset_index(drop=True)),
    "valid": Dataset.from_pandas(valid_df.reset_index(drop=True)),
     "test": Dataset.from_pandas(test_df.reset_index(drop=True))
    }
    )
dataset

dataset['train'][122]

dataset = dataset.rename_column("sentiment", "label")

model_checkpoint = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)

def tokenize_function(example):
    return tokenizer(example["review"], padding="max_length", truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

tokenized_datasets

ds = dataset.map(tokenize_function, batched=True)
ds

ds['train'][0]

"""### II. b)2. Definición de la función `compute-metrics` para la evaluación  durante el entrenamiento

A continuación, se ha definido una función, `compute_metrics` para evaluar el rendimiento de los modelos de clasificación que será invocada automáticamente por el objeto `Trainer` durante el proceso de validación con el objetivo de monitorear su calidad a lo largo del entrenamiento.

Esta función recibirá como entrada las predicciones del modelo determinado (*logits*) junto con las etiquetas reales, y transformará dichos *logits* en predicciones finales mediante la función `argmax`, que selecciona la clase con mayor probabilidad para cada ejemplo.

A partir de estas predicciones, se calcularán dos métricas clave que serán devueltas en forma de diccionario.
"""

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, predictions)
    macro_f1 = f1_score(labels, predictions, average='macro')
    return {"accuracy": acc, "macro_f1": macro_f1}

"""### II. b) 3. Aclaración sobre el proceso de entrenamiento

Dado que el proceso de entrenamiento ha debido realizarse en diversas sesiones por limitaciones de tiempo y recursos de hardware, el código a continuación mostrado se ha dividido en bloques independientes ejecutados en diferentes momentos, de ahí que la salida muestre sesiones interrumpidas inesperada o voluntariamente.

### II. b) 4. Búsqueda de hiperparámetros para el entrenamiento con RoBERTa-base: 16 combinaciones

El siguiente bloque de código se ha realizado con el objetivo de evaluar cuál de las siguientes configuraciones hiperparamétricas modélicas proporciona mejor rendimiento en la tarea de análisis de sentimiento con el corpus de reseñas de IMDb.  

En primer lugar, se ha definido el modelo base `roberta-base`, adecuado para tareas de clasificación de texto en inglés.

La función `model_init()` se encargará de instanciar un nuevo modelo preentrenado cada vez que se entrena una combinación distinta de parámetros.

A continuación, se han establecido los valores que se desean probar para tres hiperparámetros fundamentales:

1) **Número de épocas de entrenamiento** (que representa una iteración completa a través del conjunto de datos) con los valores 3, y 4.

2)**Tasa de aprendizaje (`learning_rate`)**, que controla la magnitud de los ajustes que realiza el modelo en cada paso del entrenamiento, parámetro del que se han seleccionado las tasas 2e-5, 3e-5 y 5e-5.

3) **Tasa de decaimiento de pesos (`weight_decay`)**, con valores de 0.0, 0.01 y 0.1, que actúa como regularizador para penalizar pesos excesivos y evitar el sobreajuste.

En total se han probado **16 combinaciones** diferentes de hiperparámetros.

Para recorrerlas iterativamente, se ha utilizado la función `product()` del módulo `itertools`, que permite automatizar el entrenamiento y evaluación de cada variante.

En cada iteración del bucle, se han definido los argumentos de entrenamiento mediante la clase `TrainingArguments`.

Entre los parámetros especificados se encuentra `output_dir`, que determina la ruta en la que se guardarán los resultados del modelo afinado, nombrada dinámicamente en función de los hiperparámetros usados.

El resto de los parámetros configuran aspectos como el número de épocas, la tasa de aprendizaje, el tamaño de lote, la estrategia de evaluación y guardado (ambas por época), la métrica utilizada para seleccionar el mejor modelo (macro_f1) y la opción de cargarlo al finalizar el entrenamiento.

También se ha  establecido `save_total_limit` en 1, para guardar únicamente la última versión considerada óptima, y `report_to` en *none* para evitar reportes automáticos.

Una vez definidos los argumentos y la instancia del modelo, se ha creado un objeto `Trainer`, al que se le han pasado también los datasets de entrenamiento y validación, el tokenizador y la función `compute_metrics`definida anteriormente para calcular precisión y macro F1.

Seguidamente, se ha llamado al método `train()` para iniciar el proceso de entrenamiento.

Tras entrenar el modelo, se realizará su evaluación sobre el conjunto de prueba`(ds["test"])` y se obtendrán las métricas asociadas.

#### II. b) 4. 1. Hiperparámetros evaluados el día 1:

6 combinaciones: (3, 2e-5, 0.0), (3, 2e-5, 0.01), (3, 2e-5, 0.1), (2, 3e-5, 0.0), (3, 3e-5, 0.01), (3, 3e-5, 0.1). (De esta útlima configuración solo se han entrenado dos épocas por interrupción automática del entorno de ejecución).
"""

transformer_model = 'roberta-base'

def model_init():
    return AutoModelForSequenceClassification.from_pretrained(transformer_model, num_labels=2)

epochs_list = [3]
learning_rates = [2e-5, 3e-5]
weight_decays = [0.0, 0.01, 0.1]
batch_size = 16


resultados = []

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

for num_epochs, lr, wd in itertools.product(epochs_list, learning_rates, weight_decays):
    print(f"\nEntrenando con epochs={num_epochs}, lr={lr}, weight_decay={wd}")

    model_output_dir = f"roberta_finetuned_e{num_epochs}_lr{lr}_wd{wd}".replace('.', '')

    training_args = TrainingArguments(
        output_dir=model_output_dir,
        num_train_epochs=num_epochs,
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        warmup_ratio=0.1,
        weight_decay=wd,
        eval_strategy="epoch",
        save_strategy="epoch",
        metric_for_best_model="macro_f1",
        load_best_model_at_end=True,
        save_total_limit=1,
        report_to='none'
    )

    trainer = Trainer(
        model_init=model_init,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=ds["train"],
        eval_dataset=ds["valid"],
        data_collator=data_collator
    )

    trainer.train()

    preds_output = trainer.predict(ds["test"])
    metrics = preds_output.metrics

    resultados.append({
        "Epochs": num_epochs,
        "Learning_Rate": lr,
        "Weight_Decay": wd,
        "Macro_F1": metrics.get("test_macro_f1"),
        "Accuracy": metrics.get("test_accuracy", None)
    })

"""#### II. b) 4. 2. Hiperparámetros evaluados el día 2:

5 combinaciones: (3, 5e-5, 0.0),
    (3, 5e-5, 0.01),
    (3, 5e-5, 0.1),
    (4, 2e-5, 0.0),
    (4, 2e-5, 0.01).
"""

epochs_list = [3, 4]
learning_rates = [5e-5, 2e-5]
weight_decays = [0.0, 0.01, 0.1]
batch_size = 16


resultados = []
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

for num_epochs, lr, wd in combos_pendientes:
    print(f"\nEntrenando con epochs={num_epochs}, lr={lr}, weight_decay={wd}")
    output_dir = f"roberta_finetuned_e{num_epochs}_lr{lr}_wd{wd}".replace('.', '')

    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        warmup_ratio=0.1,
        weight_decay=wd,
        eval_strategy="epoch",
        save_strategy="epoch",
        metric_for_best_model="macro_f1",
        load_best_model_at_end=True,
        save_total_limit=1,
        report_to='none'
    )

    trainer = Trainer(
        model_init=model_init,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=ds["train"],
        eval_dataset=ds["valid"],
        data_collator=data_collator
    )

    trainer.train()

    preds_output = trainer.predict(ds["test"])
    metrics = preds_output.metrics

    resultados.append({
        "Epochs":        num_epochs,
        "Learning_Rate": lr,
        "Weight_Decay":  wd,
        "Macro_F1":      metrics.get("test_macro_f1"),
        "Accuracy":      metrics.get("test_accuracy")
    })

"""#### II. b) 4. 3. Hiperparámetros evaluados el día 3:

5 combinaciones:
    (4, 2e-5, 0.1),
    (4, 3e-5, 0.0),
    (4, 3e-5, 0.01),
    (4, 3e-5, 0.1),
    (4, 5e-5, 0.0).

"""

epochs_list = [4]
learning_rates = [2e-5, 3e-5, 5e-5]
weight_decays = [0.0, 0.01, 0.1]
batch_size = 16

resultados = []
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

for num_epochs, lr, wd in combos_pendientes:
    print(f"\nEntrenando con epochs={num_epochs}, lr={lr}, weight_decay={wd}")
    output_dir = f"roberta_finetuned_e{num_epochs}_lr{lr}_wd{wd}".replace('.', '')

    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        warmup_ratio=0.1,
        weight_decay=wd,
        eval_strategy="epoch",
        save_strategy="epoch",
        metric_for_best_model="macro_f1",
        load_best_model_at_end=True,
        save_total_limit=1,
        report_to='none'
    )

    trainer = Trainer(
        model_init=model_init,
        args=training_args,
        compute_metrics=compute_metrics,
        train_dataset=ds["train"],
        eval_dataset=ds["valid"],
        data_collator=data_collator
    )

    trainer.train()

    preds_output = trainer.predict(ds["test"])
    metrics = preds_output.metrics

    resultados.append({
        "Epochs":        num_epochs,
        "Learning_Rate": lr,
        "Weight_Decay":  wd,
        "Macro_F1":      metrics.get("test_macro_f1"),
        "Accuracy":      metrics.get("test_accuracy")
    })

"""#### II. b) 4. 4. Análisis de los resultados del conjunto de validación

| Epochs | Learning Rate | Weight Decay | Macro F1 | Accuracy | Validation Loss |
|--------|----------------|---------------|----------|----------|------------------|
| 4      | 2e-5           | 0.0           | **0.958995** | 0.959000 | 0.223477         |
| 4      | 2e-5           | 0.01          | 0.957600 | 0.957600 | 0.234824         |
| 4      | 2e-5           | 0.1           | 0.957396 | 0.957400 | 0.239873         |
| 4      | 3e-5           | 0.01          | 0.954793 | 0.954800 | 0.207851         |
| 3      | 5e-5           | 0.01          | 0.952594 | 0.952600 | 0.203274         |
| 4      | 3e-5           | 0.0           | 0.952194 | 0.952200 | 0.263150         |
| 3      | 5e-5           | 0.0           | 0.950799 | 0.950800 | 0.200558         |

Tras el análisis de las métricas del conjunto de validación, se observa que las configuraciones con mejor rendimiento comparten determinados patrones: el número de épocas óptimo es 4; la tasa de aprendizaje *(learning rate)* aporta mejores resultados con valores bajos, concretamente 2e-5, mientras que por lo que respecta a la regularización, un *weight decay* bajo o nulo (0.0 o 0.01) es más efectivo.

La configuración hiperparamétrica óptima identificada en este experimento es: `Epochs = 4, Learning Rate = 2e-5, Weight Decay = 0.0` con una Macro-F1 excelente: 0. 958995 que demuestra que un modelo con una arquitectura basada en transformadores entrenado sobre un dataset binario bien balanceado y, en general, bien etiquetado puede alcanzar una precisión casi absoluta.

### II. b) 5. Evaluación del mejor modelo sobre el conjunto de prueba

Posteriormente, tras su reentrenamiento, se ha evaluado el mejor modelo afinado sobre el conjunto de prueba para comprobar su capacidad de generalización ante nuevos datos.
"""

batch_size = 16
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

training_args = TrainingArguments(
    output_dir="roberta_finetuned_e4_lr2e-5_wd0",
    num_train_epochs=4,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    warmup_ratio=0.1,
    weight_decay=0.0,
    eval_strategy="epoch",
    save_strategy="epoch",
    metric_for_best_model="macro_f1",
    load_best_model_at_end=True,
    save_total_limit=1,
    report_to='none'
)

trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=ds["train"],
    eval_dataset=ds["valid"],
    data_collator=data_collator
)

trainer.train()

metrics_test = trainer.predict(ds["test"]).metrics
print("Test Accuracy:", metrics_test["test_accuracy"])
print("Test Macro F1:", metrics_test["test_macro_f1"])
print("Test Loss:", metrics_test["test_loss"])

"""La evaluación final del modelo `roberta-base` *fine-tuned* sobre el conjunto de test demuestra un excelente rendimiento y capacidad de generalización sobre reseñas no contempladas anteriormente.

### II. b) 6. Análisis cuantitativo y cualitativo del mejor modelo afinado

#### II. b) 6. 1. Análisis de errores mediante una matriz de confusión
"""

predicciones_test = trainer.predict(ds["test"])
y_pred = predicciones_test.predictions.argmax(axis=1)
y_true = predicciones_test.label_ids

disp.plot(cmap="Purples")
plt.title("Matriz de confusión - RoBERTa fine-tuned (test)")
plt.show()

"""La matriz de confusión muestra que el modelo RoBERTa *fine-tuned*, evaluado sobre el conjunto de prueba, clasifica correctamente 2403 de 2500 ejemplos negativos (96.12%) y 2425 de 2500 ejemplos positivos (97%).

Se refleja, de este modo, que los falsos positivos y los falsos negativos están distribuidos equilibradamente, hecho que manifiesta que el modelo no presenta sesgos hacia ninguna de las clases.

#### II. b) 6. 2. Análisis cualitativo: predicciones de sentimiento del mejor modelo *Roberta-baase fine-tuned* (Epochs = 4, Learning Rate = 2e-5, Weight Decay = 0.0)
"""

predictions_output = trainer.predict(ds["test"])
y_pred_probs = predictions_output.predictions


print("Shape de las predicciones:", y_pred_probs.shape)


if y_pred_probs.shape[1] == 2:

    y_pred = np.argmax(y_pred_probs, axis=1)
else:

    y_pred = (y_pred_probs > 0.5).astype(int).flatten()


texts = ds["test"]["review"]
labels = ds["test"]["label"]


df_test = pd.DataFrame({
    "review": texts,
    "sentiment": labels,
    "pred_transformer": y_pred
})

df_test["acierto_transformer"] = df_test["pred_transformer"] == df_test["sentiment"]


pd.set_option('display.max_colwidth', None)

print("\nEjemplos clasificados diferentemente:")
display(df_test[df_test["acierto_transformer"] == False][["review", "sentiment", "pred_transformer"]].sample(2, random_state=42))

print("\nEjemplos clasificados igualmente:")
display(df_test[df_test["acierto_transformer"] == True][["review", "sentiment", "pred_transformer"]].sample(2, random_state=42))

"""#### II. b) 6. 2. 1. Reseñas 1590 y 1012

A pesar de la dificultad clasificatoria de la reseña 1590, debido a su carácter resumidor con un tono neutro, desde mi perspectiva, la etiqueta correcta es la asignada por el modelo *Transformer*, *positive*, y no por el etiquetador del dataset, pues se puede entrever una polaridad positiva del sentimiento en adjetivos valorativos como *interesting*, que, asimismo, por contagio coocurrencial, connota positivamente a *weird*: "that is where it gets interesting and weird".

Además, la oración final, referente al protagonista, contribuye a desambiguar el tono neutro inicial: "Lugosi was marvelous as the skulking killer".

Por otro lado, el texto de la crítica 1012 contiene múltiples indicios explícitos de valoración favorable hacia la serie Sabrina, *the Teenage Witch*, así como hacia la actriz principal Melissa Joan Hart, de ahí que la clasificación del modelo coincida con la del dataset: *positive*

De este modo, la oración con sustantivos metafóricos que connotan sentimiento claramente positivo como *heart y soul* en "Hart is the show's heart & soul", los adjetivos valorativos positivos precedidos de intensificadores, *very funny* : "Sabrina, the Teenage Witch is quite moving and very funny", la expresión de un deseo explícito de continuidad: "I only wish there were some newer episodes that we could all enjoy" y el cierre, que incluye la puntuación de "10/10", refuerzan inequívocamente el tono positivo del texto.

#### II. b) 6. 3. Análisis de ejemplos con mayor error de predicción
"""

model.eval()

losses = []
predictions = []
review_texts = []
true_labels = []

for i in range(len(ds['test'])):
    sample = ds['test'][i]

    inputs = {
        "input_ids": torch.tensor(sample["input_ids"]).unsqueeze(0).to(model.device),
        "attention_mask": torch.tensor(sample["attention_mask"]).unsqueeze(0).to(model.device)
    }

    label = torch.tensor(sample["label"]).unsqueeze(0).to(model.device)
    true_labels.append(sample["label"])

    with torch.no_grad():
        outputs = model(**inputs, labels=label)

    loss = outputs.loss.item()
    logits = outputs.logits
    pred = torch.argmax(logits, dim=-1).item()

    losses.append(loss)
    predictions.append(pred)


    review_texts.append(test_df.iloc[i]["review"])


df_resultados = pd.DataFrame({
    "review_body": review_texts,
    "true_label": true_labels,
    "predicted_label": predictions,
    "loss": losses
})


label_map = {0: "negative", 1: "positive"}
df_resultados["true_label_text"] = df_resultados["true_label"].map(label_map)
df_resultados["predicted_label_text"] = df_resultados["predicted_label"].map(label_map)


df_resultados = df_resultados.sort_values(by="loss", ascending=False)
pd.set_option('display.max_colwidth', None)

df_resultados.head(10)

"""#### II. b) 6. 3. 1.  Instancia con el índice 358

La reseña 358 presenta expresiones con una polaridad positiva evidente sin ninguna ambigüedad ni matización: *sleek, sexy movie is a must-see, defines the word masterpiece* y *wonderful insight*, los adjetivos valorativos son, además de connotativamente positivos, superlativos, *the uniqueness of this film*, de donde se deduce que, probablemente, el error del etiquetador del dataset se deba a la catalogación. Es esta incorrección la que explica el alto valor de la pérdida del modelo para esta muestra, 7.72 y constituye un ejemplo de cómo las métricas cuantitavias no son suficientes en las evaluaciones modélicas.

#### II. b) 6. 3. 2.  Instancia con el índice 4817

Frente a la crítica cinematográfica previamente analizada, la numerada como 4817 es un ejemplo de error automático modélico debido a la polaridad, en ocasiones ambigua, del sentimiento de la reseña.

Aunque la crítica contiene juicios claramente negativos hacia numerosos aspectos de la película (actuación, dirección, guion, etc.), también incluye descripciones positivas del argumento original, así como un tono inicialmente admirativo.

Esta mezcla de polaridad positiva y negativa, observable, por ejemplo, en expresiones como *it's a beautiful, seductive story*, frente a *the acting was poor* o *it was boring* puede haber causado confusión en el modelo y constituir la explicación de la predicción positiva cuando la etiqueta real es negativa.

El valor de la función de pérdida para esta muestra es alto, de 7.71, y revela los límites de los clasificadores automáticos ante juicios mixtos.

#### II. b) 6. 3. 3.  Instancias positivas clasificadas como negativas

 Tras analizarse los falsos positivos, se ha estudiado algún caso inverso: instancias etiquetadas como positivas clasificadas por el modelo como negativas.
"""

errores_pn = df_resultados[
    (df_resultados["true_label"] == 1) &
    (df_resultados["predicted_label"] == 0)
]
errores_pn = errores_pn.sort_values(by="loss", ascending=False)

errores_pn.head()

"""#### II. b) 6. 3. 3. 1.  Instancia con el índice 3303

Una revisión detallada de las predicciones muestra, una vez más, que algunas discrepancias entre las salidas del modelo y las etiquetas reales no necesariamente reflejan fallos de clasificación, sino  errores en el conjunto de datos.

Es el caso de la reseña 3303, etiquetada en el dataset como positiva y  acertadamente por el modelo como negativa.

En la reseña se observan juicios de valor con construcciones superlativas con  connotación negativa como *I think it is almost Adam's worst film* (aunque el adverbio *almost* suaviza levemente el juicio), o ...*this film lack depth and a decent story line and deserves to be in the bottom 100*.

Si bien ciertas expresiones como *when Dickie gets thrown off the boat it is so funny* o *I like it when he flips everyone off* denotan, mediante adjetivos valorativos con connotación positiva o a través de verbos como *like*, que expresan agrado, una polaridad positiva, el tono global de la reseña es negativo y crítico.

#### II. b) 6. 4. Análisis de ejemplos con LIME
"""

explainer = LimeTextExplainer(class_names=["negative", "positive"])

def predictor(texts):
    all_probs = []
    lime_batch_size = 32

    for i in range(0, len(texts), lime_batch_size):
        batch_texts = texts[i : i + lime_batch_size]
        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors="pt").to("cpu")
        with torch.no_grad():
            logits = model(**inputs).logits
            probs = torch.softmax(logits, dim=-1).cpu().numpy()
            all_probs.append(probs)

    return np.concatenate(all_probs, axis=0)

errores_a_analizar = [4686, 540]

for idx in errores_a_analizar:
    texto = test_df["review"].iloc[idx]
    true_label = test_df["sentiment"].iloc[idx]

    explicacion = explainer.explain_instance(texto, predictor, num_features=10, num_samples=1000)

    print(f"\nEjemplo índice {idx} - Etiqueta real: {'positive' if true_label == 1 else 'negative'}")
    print(texto)
    explicacion.show_in_notebook()

"""#### II. b) 6. 4. 1.  Instancia 4686

La etiqueta real del conjunto de datos para esta instancia es *negative*, mientras que el modelo predice la clase *positive* con una probabilidad de 1.

El contenido textual revela que la predicción del modelo es coherente pues, aunque la reseña del videojuego no sea positiva en grado máximo, presenta expresiones como *quite engaging*, en relación con la trama, *ground-breaking for their time*, en alusión a *the graphics*, *memorable* en referencia a *the music and characters* y sustantivos que connotan sentimiento positivo como *entertainment* en *dozens of hours of entertainment*.

Según LIME, palabras que han contribuido a esta determinación son *enjoy* y *memorable*.

Sin embargo, destaca el hecho de que *lacking* también aparezca con una ponderación positiva que, probablemente, se explique por la importancia concedida al contexto, en el que adverbio *perhaps* matiza el significado del sustantivo denostador *lacking*.

Con ponderación negativa aparecen tokens como *sorry* y palabras neutrales como *vector* o *graphics*, cuya aportación a la predicción es finalmente nula.

#### II. b) 6. 4. 2.  Instancia 540

De nuevo el ejemplo 540 representa un caso especialmente interesante para el análisis con LIME, ya que el modelo predice *negative* mientras que la etiqueta del dataset es erróneamente *positive*.

La reseña contiene valoraciones muy negativas, expresadas en un tono sarcástico mediante una crítica directa: *this film is terrible, this film is awful, the main character... constantly mumbles, Best British comedy? Please..*. Esta contradicción entre contenido y etiqueta refleja, una vez más, un error en el etiquetado del dataset, y justifica el alto valor de la función de pérdida (7.55).

Adjetivos valorativos como *awful y terrible* han sido los más influyentes para que el modelo predijera la clase negativa, frente al verbo auxiliar *do* o a la *stopword* *the* con, prácticamente, valor nulo en la tendencia positiva
y sin influencia final en el porcentaje de confianza del modelo, 100%, hacia la predicción negativa.

##7. Valoraciones finales

###7. 1.  Comparativa de los modelos evaluados, valoración del dataset y conclusión

<span style="font-size: 110%">

| Modelo                           | Macro F1 | Accuracy | Notas adicionales                                 |
|---------------------------------------------|----------|----------|---------------------------------------------------|
| Lexicón SentiWordNet (todos los synsets - suma/media/heurísticas) | 0.69     | 0.69     | Ventanas de 3, 4 y 5 tokens  |
| Regresión logística (TF-IDF + n-gramas)      | 0.9180   | 0.9180   | RidgeClassifier con bigramas y trigramas         |
| LSTM unidireccional + ReLU                   | 0.8832   | 0.8832   | Con *embeddings* GloVe                             |
| RoBERTa fine-tuned (4 epochs, 2e-5, wd=0.0)  | **0.9656** | **0.9656** | Evaluado en test                                  |

</span>

A lo largo de este experimento se han diseñado, probado y evaluado numerosas configuraciones de modelos para la clasificación binaria de Análisis de Sentimiento sobre el conjunto de datos de reseñas cinematográficas IMDb, conjunto del que se han planteado las siguientes consideracioness:

a) Por lo que concierne a la distribución de clases positivas y negativas,
su balance equilibrado facilita la evaluación de modelos sin sesgo hacia una categoría específica.

b) Dado que las reseñas requieren un buen Procesamiento del Lenguaje Natural para optimizar la calidad del dataset, debiera haberse aplicado la eliminación de las etiquetas de lenguaje de Programación HTML detectadas en el trabajo, para evitar ruido en el conjunto de entrenamiento de los modelos.

c) Debe tenerse en cuenta que el dataset está conformado por críticas cinematográficas, por lo que esta tipología textual restringe su aplicabilidad a determinados dominios por la presencia de ciertos rasgos lingüísticos como: la función expresiva del lenguaje, el léxico técnico cinematográfico, el registro no formal, etc.

d) Aunque la valoración global del dataset es positiva, debe tenerse en cuenta que no siempre las clases están correctamente asignadas a las reseñas correspondientes.

Por otro lado, mediante el experimento se ha hecho un recorrido por distintas técnicas de Aprendizaje Automático aplicadas al Análisis de Sentimiento.

De esta manera, se ha partido de perspectivas clásicas, configuradas mediante reglas heurísticas con lexicones; se ha pasado, después, por modelos de Regresión Logística, basados en vectorizadores frecuenciales como *Term Frequency-Inverse Document Frequency* (TF-IDF), construidos sobre bi y trigramas; se ha experimentado, además, con modelos de Aprendizaje Profundo, sustentados en redes neuronales y, por último, se han entrenado modelos basados en arquitecturas conformadas por Transformers.

Este recorrido ha desvelado, tanto a través de métricas cuantitativas, como de análisis cualitativos que, si bien los modelos clásicos basados en reglas heurísticas han sido poco precisos en la clasificación de reseñas, mayor complejidad modélica no siempre implica mejor rendimiento, especialmente si los datos están bien balanceados y la clasificación es binaria y, de esta manera, se ha observado cómo modelos de Regresión Logística han arrojado mejores resultados que otros basados en arquitecturas más complejas como LSTM con *embeddings* preentrenados y con varias capas neuronales.

De este modo, este caso ilustra cómo el análisis del dataset y de su estructura han de guiar en la elección de un clasificador para tareas del Procesamiento del Lenguaje Natural (PLN) como el Análisis del Sentimiento y evitar así el empleo de recursos no solo innecesarios, sino también menos eficientes.

Asimismo, el experimento ha probado por qué las redes neuronales basadas en transformadores han revolucionado el modelado automático del lenguaje, pues, al hecho de que las métricas hayan evidenciado valores de precisión y Macro-F1 próximos al 97%, con una función de pérdida aproximada del 0.71, se suma el que los análisis cualitativos de predicciones, así como de pérdida por predicción y mediante herramientas como LIME hayan desvelado que esas métricas cuantitativas no son del todo representativas y, en realidad, existen numerosos casos en los que el modelo ha predicho la categoría con mayor precisión que el etiquetador del dataset, por lo que se propone el fomento de estudios cualitativos para determinar, junto a los cuantitativos, el rendimiento real de los modelos analizados.

En definitiva, se concluye que, si bien el experimento corrobora cómo modelos del lenguaje como RoBERTa (*Robustly Optimized BERT Approach)* han supuesto un avance significativo en los estudios lingüísticos computacionales, no deben despreciarse perspectivas tradicionales, como la Regresión Logística, cuyo valor, 0.918, de Macro-F1 permite considerarlo, en numerosos casos, una eficiente solución para problemas de clasificación binaria de Análisis de Sentimiento.
"""